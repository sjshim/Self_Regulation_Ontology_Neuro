{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cb1285c4bd1a>:15: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from os import path\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import copy\n",
    "from math import ceil, floor\n",
    "\n",
    "%matplotlib inline\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/groups/russpold/uh2_analysis/Self_Regulation_Ontology_fMRI_2021/fmri_analysis/scripts/aim1_2ndlevel_regressors\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Max_SSD = 1\n",
    "Min_SSD = 0\n",
    "MAX_RT_STOP_TASK = 2.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths / Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OAK = '/oak/stanford/groups/russpold' # CHANGE\n",
    "#OAK = '/Volumes/russpold/' # CHANGE\n",
    "BIDS_dir = path.join(OAK, 'data','uh2','aim1', 'BIDS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tasks to be looked at\n",
    "tasks = ['ANT', 'discountFix', 'DPX', 'motorSelectiveStop', 'stopSignal', \\\n",
    "         'twoByTwo', 'stroop', 'WATT', 'CCTHot', 'surveyMedley']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped subjects - if you find subjects with crappy beh data, put them in here as 'poor_performance_subjective_rating'\n",
    "dropped_subjects = {\n",
    "    's638': {'DPX': ['poor_performance_subjective_rating']},\n",
    "    's586': {'DPX': ['poor_performance_subjective_rating'],\n",
    "            'motorSelectiveStop': ['poor_performance_subjective_rating']},\n",
    "    's650': {'ANT': ['poor_performance_subjective_rating'],\n",
    "             'DPX': ['poor_performance_subjective_rating'],\n",
    "             'stroop': ['poor_performance_subjective_rating']},\n",
    "    's623': {'DPX': ['poor_performance_subjective_rating'],\n",
    "             'twoByTwo': ['poor_performance_subjective_rating'],\n",
    "             'surveyMedley' : ['poor_performance_subjective_rating']},\n",
    "    's608': {'stroop': ['poor_performance_subjective_rating']},\n",
    "    's172': {'WATT': ['poor_performance_subjective_rating']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {} #dict for storing task event file data \n",
    "for task in tasks: \n",
    "    file_dict[task] = glob(path.join(BIDS_dir, 'sub-s*', 'ses-*', 'func', f'*{task}*_events.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_summary(data_df, conditions, condition_column='trial_type'): \n",
    "    \"\"\"\n",
    "    calculates mean data for reaction time by condition\n",
    "    \n",
    "    Input: \n",
    "    - data_df::pandas df of the beh data across all subjects (1 large df of all subjects beh data)\n",
    "    - conditions::list of conditions of interest for each task\n",
    "    - condition_column::the column of the df we are interested in focusing on in terms of task conditions\n",
    "    \n",
    "    Output:\n",
    "    - rt_df::pandas df of rt measures\n",
    "    \"\"\"\n",
    "    \n",
    "    # creates an empty df to input all the rt data for task conditions in question accross all subjects\n",
    "    rt_df = pd.DataFrame()\n",
    "    index = data_df['worker_id'].unique()\n",
    "    \n",
    "    columns = []\n",
    "    if 'attention_network_task' in data_df['experiment_exp_id'].unique():\n",
    "        for cond in conditions:\n",
    "            columns.append(f'spatial_{cond}_rt')\n",
    "            columns.append(f'double_{cond}_rt')\n",
    "    else:    \n",
    "        for cond in conditions:\n",
    "            columns.append(f'{cond}_rt')\n",
    "            \n",
    "    rt_df = pd.DataFrame(index = index, columns = columns)\n",
    "                                        \n",
    "    # filters through every participant\n",
    "    for subj in data_df['worker_id'].unique():\n",
    "        \n",
    "        # obtains avg rt for each subject across conditions\n",
    "        for cond in conditions: \n",
    "    \n",
    "            if 'ward_and_allport' in data_df['experiment_exp_id'].unique():\n",
    "                subj_df = data_df.loc[(data_df.loc[:,'worker_id'] == subj) & (data_df.loc[:,'planning'] == 1)]\n",
    "            else:\n",
    "                subj_df = data_df.loc[(data_df.loc[:,'worker_id'] == subj)]\n",
    "            \n",
    "            if 'attention_network_task' in data_df['experiment_exp_id'].unique():\n",
    "                double_mean_rt = subj_df.loc[(subj_df.loc[:, condition_column] == cond) & (subj_df.loc[:, 'cue'] == 'double')]['response_time'].mean()\n",
    "                spatial_mean_rt = subj_df.loc[(subj_df.loc[:, condition_column] == cond) & (subj_df.loc[:, 'cue'] == 'spatial')]['response_time'].mean()\n",
    "                rt_df.loc[(subj, f'double_{cond}_rt')] = double_mean_rt\n",
    "                rt_df.loc[(subj, f'spatial_{cond}_rt')] = spatial_mean_rt\n",
    "            else:\n",
    "                mean_rt = subj_df.loc[subj_df.loc[:, condition_column] == cond]['response_time'].mean()\n",
    "                rt_df.loc[(subj, f'{cond}_rt')] = mean_rt\n",
    "            \n",
    "            # obtains avg SSD for stop tasks\n",
    "            if 'attention_netwstop_signalork_task' in data_df['experiment_exp_id'].unique() or 'motor_selective_stop_signal' in data_df['experiment_exp_id'].unique():\n",
    "                rt_df.loc[(subj, 'mean_SSD')] = subj_df.loc[:, 'SS_delay'].mean()\n",
    "            \n",
    "        \n",
    "    return rt_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_summary(data_df, conditions, condition_column='trial_type'): \n",
    "    \"\"\"\n",
    "    calculates summary data for task accuracy by condition\n",
    "    obtains stop success and min/max ssd for stop signal and motor stop tasks\n",
    "    obtains ommission rate for each task by condition\n",
    "    \n",
    "    Input: \n",
    "    - data_df::pandas df of the beh data across all subjects (1 large df of all subjects beh data)\n",
    "    - conditions::list of conditions of interest for each task\n",
    "    - condition_column::the column of the df we are interested in focusing on in terms of task conditions\n",
    "    \n",
    "    Output:\n",
    "    - acc_df::pandas df of accuracy measures\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocessing: creates an empty df to input all the acc data for task conditions in question accross all subjects\n",
    "    acc_df = pd.DataFrame()\n",
    "    index = data_df['worker_id'].unique()\n",
    "    \n",
    "    columns = []\n",
    "    if data_df['experiment_exp_id'].unique() == 'attention_network_task':\n",
    "        for cond in conditions:\n",
    "            columns.append(f'spatial_{cond}_acc')\n",
    "            columns.append(f'double_{cond}_acc')\n",
    "    else:    \n",
    "        for cond in conditions:\n",
    "            columns.append(f'{cond}_acc')\n",
    "        \n",
    "        columns.append('omission_rate')\n",
    "        columns.append('overall_omission_rate')\n",
    "        columns.append('truncation')\n",
    "    \n",
    "    acc_df = pd.DataFrame(index = index, columns = columns)\n",
    "                                    \n",
    "    # filters through every individual   \n",
    "    for subj in data_df['worker_id'].unique():\n",
    "        for cond in conditions:\n",
    "            \n",
    "            # obtains accuracy for conditions, ANT task gets treated differently because we seperate by cue as well\n",
    "            if data_df['experiment_exp_id'].unique() == 'attention_network_task':\n",
    "                subj_df_double = data_df.loc[(data_df.loc[:,'worker_id'] == subj) & (data_df[condition_column]==cond) & (data_df['cue']=='double')]\n",
    "                subj_df_spatial = data_df.loc[(data_df.loc[:,'worker_id'] == subj) & (data_df[condition_column]==cond) & (data_df['cue']=='spatial')]\n",
    "                double_acc = sum(subj_df_double['correct']) / len(subj_df_double['correct']) \n",
    "                spatial_acc = sum(subj_df_spatial['correct']) / len(subj_df_spatial['correct']) \n",
    "                acc_df.loc[(subj, f'spatial_{cond}_acc')] = spatial_acc\n",
    "                acc_df.loc[(subj, f'double_{cond}_acc')] = double_acc\n",
    "            else:\n",
    "                if data_df['experiment_exp_id'].unique() == 'dot_pattern_expectancy':\n",
    "                    subj_df = data_df.loc[(data_df.loc[:,'worker_id'] == subj) & (data_df[condition_column]==cond) & (data_df['trial_id']=='probe')]\n",
    "                else:\n",
    "                    subj_df = data_df.loc[(data_df.loc[:,'worker_id'] == subj) & (data_df[condition_column]==cond)]\n",
    "                if cond in subj_df[condition_column].unique():\n",
    "                    acc = sum(subj_df['correct']) / len(subj_df['correct']) \n",
    "                    acc_df.loc[(subj, f'{cond}_acc')] = acc\n",
    "                else:\n",
    "                    acc_df.loc[(subj, f'{cond}_acc')] = 0\n",
    "            \n",
    "            # obtains min/max ssd and stop success rate for stop tasks\n",
    "            if data_df['experiment_exp_id'].unique() == 'stop_signal' or  data_df['experiment_exp_id'].unique() == 'motor_selective_stop_signal':\n",
    "                beh_df = data_df.loc[(data_df.loc[:,'worker_id'] == subj)]\n",
    "                \n",
    "                if data_df['experiment_exp_id'].unique() == 'stop_signal':\n",
    "                    stop_trials = beh_df.loc[beh_df['trial_type'].isin(['stop_failure', 'stop_success'])]\n",
    "                    acc_df.loc[(subj, 'max_SSD_count')] = (stop_trials.SS_delay.values == Max_SSD).sum()\n",
    "                    acc_df.loc[(subj, 'min_SSD_count')] = (stop_trials.SS_delay.values == Min_SSD).sum()\n",
    "                    acc_df.loc[(subj, 'mean_SSD')] = (stop_trials.SS_delay.values).mean()\n",
    "                    acc_df.loc[(subj, 'SSRT')] = calc_SSRT(beh_df, task = 'stop_signal')\n",
    "                    \n",
    "                elif data_df['experiment_exp_id'].unique() == 'motor_selective_stop_signal':\n",
    "                    stop_trials = beh_df.loc[beh_df['trial_type'].isin(['crit_stop_success', 'crit_stop_failure'])]\n",
    "                    acc_df.loc[(subj, 'max_SSD_count')] = (stop_trials.SS_delay.values == Max_SSD).sum()\n",
    "                    acc_df.loc[(subj, 'min_SSD_count')] = (stop_trials.SS_delay.values == Min_SSD).sum()\n",
    "                    acc_df.loc[(subj, 'mean_SSD')] = (stop_trials.SS_delay.values).mean()\n",
    "                    acc_df.loc[(subj, 'SSRT')] = calc_SSRT(beh_df, task = 'motor_selective_stop_signal')\n",
    "                    \n",
    "                    crit_go_trials = beh_df[(beh_df.trial_type == 'crit_go')]\n",
    "                    noncrit_nosignal_trials = beh_df[(beh_df.trial_type == 'noncrit_nosignal')]\n",
    "                    noncrit_signal_trials = beh_df[(beh_df.trial_type == 'noncrit_signal')]\n",
    "                    \n",
    "                    acc_df.loc[(subj, 'noncrit_signal_omission')] = (noncrit_signal_trials.key_press.values == -1).sum() / len(noncrit_signal_trials)\n",
    "                    acc_df.loc[(subj, 'noncrit_nosignal_omission')] = (noncrit_nosignal_trials.key_press.values == -1).sum() / len(noncrit_nosignal_trials)\n",
    "                    acc_df.loc[(subj, 'crit_go_omission')] = (crit_go_trials.key_press.values == -1).sum() / len(crit_go_trials)\n",
    "                    \n",
    "                if set(data_df[data_df['worker_id'] == subj].key_press) == {-1.0, 71.0, 89.0}:\n",
    "                    if data_df['experiment_exp_id'].unique() == 'stop_signal':\n",
    "                        stop_success = len(beh_df[beh_df.trial_type == 'stop_success'])\n",
    "                        stop_failure = len(beh_df[beh_df.trial_type == 'stop_failure'])\n",
    "                        \n",
    "                    elif data_df['experiment_exp_id'].unique() == 'motor_selective_stop_signal':\n",
    "                        stop_success = len(beh_df[beh_df.trial_type == 'crit_stop_success'])\n",
    "                        stop_failure = len(beh_df[beh_df.trial_type == 'crit_stop_failure'])\n",
    "\n",
    "                    total = stop_success + stop_failure\n",
    "                    stop_success_rate = stop_success / total\n",
    "                    acc_df.loc[(subj, 'stop_success_rate')] = stop_success_rate\n",
    "                else:\n",
    "                    acc_df.loc[(subj, 'stop_success_rate')] = 0\n",
    "        \n",
    "        # obtains omission rate for current data\n",
    "        subj_df = data_df.loc[(data_df.loc[:,'worker_id'] == subj)]\n",
    "        \n",
    "        acc_df.loc[(subj, 'omission_rate')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "        \n",
    "        # obtains omission rate from any previous iteration in the event the data was truncated\n",
    "        if 'overall_omission_rate' in subj_df.columns:\n",
    "            acc_df.loc[(subj, 'overall_omission_rate')] = subj_df['overall_omission_rate'].unique()[0]\n",
    "        else:\n",
    "            acc_df.loc[(subj, 'overall_omission_rate')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "        \n",
    "        # obtains truncation rate if data was truncated\n",
    "        if 'truncation' in subj_df.columns:\n",
    "            acc_df.loc[(subj, 'truncation')] = subj_df['truncation'].unique()[0]\n",
    "        else:\n",
    "            acc_df['truncation'] = np.nan\n",
    "            \n",
    "    return acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_SSRT(df, task = None, max_rt = MAX_RT_STOP_TASK):\n",
    "    if task == 'stop_signal':\n",
    "        go_trials = df.query('trial_type == \"go\"')\n",
    "        stop_trials = df.query('trial_type == \"stop_success\" or trial_type == \"stop_failure\"')\n",
    "        go_replacement_df = go_trials.where(go_trials['response_time'] != -1, max_rt)\n",
    "        sorted_go = go_replacement_df.response_time.sort_values(ascending = True)\n",
    "        prob_stop_failure = (1-stop_trials.stopped.mean())\n",
    "        nth = prob_stop_failure*(len(sorted_go)-1)\n",
    "        index = [floor(nth), ceil(nth)]\n",
    "        SSRT = sorted_go.iloc[index].mean() - stop_trials.SS_delay.mean()\n",
    "        return SSRT\n",
    "    \n",
    "    elif task == 'motor_selective_stop_signal':\n",
    "        \n",
    "        go_trials = df.query('trial_type == \"crit_go\"')\n",
    "        stop_trials = df.query('trial_type == \"crit_stop_failure\" or trial_type == \"crit_stop_success\"')\n",
    "        go_replacement_df = go_trials.where(go_trials['response_time'] != -1, max_rt)\n",
    "        sorted_go = go_replacement_df.response_time.sort_values(ascending = True)\n",
    "        prob_stop_failure = (1-stop_trials.stopped.mean())\n",
    "        nth = prob_stop_failure*(len(sorted_go)-1)\n",
    "        index = [floor(nth), ceil(nth)]\n",
    "        SSRT = sorted_go.iloc[index].mean() - stop_trials.SS_delay.mean()\n",
    "        return SSRT      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omission_rate(df, task):\n",
    "    \"\"\"\n",
    "    function to calc the rate of ommisions within the data\n",
    "    \n",
    "    Input: \n",
    "    - df::pandas df of the beh data for an individual subject\n",
    "    - task::str::task that the data belongs to\n",
    "    \n",
    "    Output:\n",
    "    - non_response_rate::int::scalar value representing non-response rate of subject for task\n",
    "    \"\"\"\n",
    "    \n",
    "    # checks that the data is not empty or missing\n",
    "    if df.empty == False: \n",
    "        \n",
    "        # obtains omission or non-response rate for specific task\n",
    "        if (task == 'stop_signal') or (task == 'stopSignal'):\n",
    "            go_trials = df[(df.trial_type == 'go')]\n",
    "            non_response_rate = len(go_trials[go_trials.key_press == -1]) / len(df[df.trial_type == 'go'])\n",
    "        elif (task == 'motor_selective_stop_signal') or (task == 'motorSelectiveStop'):\n",
    "            crit_go_trials = df[(df.trial_type == 'crit_go')]\n",
    "            noncrit_nosignal_trials = df[(df.trial_type == 'noncrit_nosignal')]\n",
    "            noncrit_signal_trials = df[(df.trial_type == 'noncrit_signal')]\n",
    "            go_trials = pd.concat([crit_go_trials, noncrit_nosignal_trials, noncrit_signal_trials])\n",
    "            non_response_rate = len(go_trials[go_trials.key_press == -1]) / len(df[(df.trial_type != 'crit_stop_success') & (df.trial_type != 'crit_stop_failure')])\n",
    "        elif (task == 'survey_medley') or (task == 'surveyMedley'):\n",
    "            non_response_rate = len(df[df.coded_response == 0]) / len(df.coded_response)\n",
    "        elif (task == 'columbia_card_task_fmri') or (task == 'CCTHot'):\n",
    "            non_response_rate = len(df[(df['action'] != -1) & (df['key_press'] == -1)]) / len(df[df['action'] != -1])\n",
    "        elif (task == 'ward_and_allport') or (task == 'WATT'):\n",
    "            non_response_rate = len(df.loc[(df['trial_id'] != 'feedback') & (df['key_press'] == -1)]) / len(df[df['trial_id'] != 'feedback'])            \n",
    "        else:\n",
    "            non_response_rate = len(df[df.key_press == -1]) / len(df.key_press)\n",
    "            \n",
    "    else:\n",
    "        non_response_rate = 1\n",
    "        \n",
    "    return non_response_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCTHot_EV_acc(task_df):\n",
    "    \"\"\"\n",
    "    Function to calc accuracy for CCTHot task\n",
    "    We compute accuracy as the proportion of rewarded trials.\n",
    "    We also compute ommision rate for the task data\n",
    "    \n",
    "    Input: \n",
    "    - task_df::pandas dataframe:: beh data for task accross all subjects\n",
    "    \n",
    "    Output:\n",
    "    - acc_df::pandas df of accuracy measures for CCTHot\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocessing\n",
    "    task_df['EV_new'] = task_df['gain_amount']*task_df['gain_probability'] + task_df['loss_amount']*task_df['loss_probability']\n",
    "    task_df['correct'] = np.nan\n",
    "    # incorrect to draw card with negative EV, end round with positive EV\n",
    "    task_df.loc[(task_df.action=='draw_card') & (task_df.EV_new < 0), 'correct'] = 0\n",
    "    task_df.loc[(task_df.action=='end_round') & (task_df.EV_new >= 0), 'correct'] = 0\n",
    "    # correct to draw card with positive EV, end round with negative EV\n",
    "    task_df.loc[(task_df.action=='draw_card') & (task_df.EV_new >= 0), 'correct'] = 1\n",
    "    task_df.loc[(task_df.action=='end_round') & (task_df.EV_new < 0), 'correct'] = 1\n",
    "    \n",
    "    # creates df to store acc data\n",
    "    acc_df = pd.DataFrame()\n",
    "    index = task_df['worker_id'].unique()\n",
    "    acc_df = pd.DataFrame(index = index, columns = ['acc'])\n",
    "    \n",
    "    # iterate through every subject to obtain individual acc/ommission rate\n",
    "    for subj in task_df.worker_id.unique():\n",
    "        acc_df.loc[(subj, 'acc')] = task_df.loc[task_df.worker_id == subj, 'correct'].mean()\n",
    "        \n",
    "        subj_df = task_df.loc[(task_df.loc[:,'worker_id'] == subj)]\n",
    "        \n",
    "        # overall omission rate doesn't need to be included, so I may remove and see if the code works\n",
    "        acc_df.loc[(subj, 'CCTHot_overall_omission_rate')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "        acc_df.loc[(subj, 'omission_rate')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "        \n",
    "        # truncation is not being done on CCTHot due to the difference in accuracy measures we obtain\n",
    "        # included due to errors flaggin when other functions iterate through the final output - may take out\n",
    "        acc_df.loc[(subj, 'CCTHot_truncation')] = np.nan\n",
    "        \n",
    "    return acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WATT_acc(task_df):\n",
    "    \"\"\"\n",
    "    Function to calc accuracy for WATT task\n",
    "    We compute accuracy as extra moves taken.\n",
    "    We also compute ommision rate for the task data\n",
    "    \n",
    "    Input: \n",
    "    - task_df::pandas dataframe:: beh data for task accross all subjects\n",
    "    \n",
    "    Output:\n",
    "    - acc_df::pandas df of accuracy measures for WATT\n",
    "    \"\"\"\n",
    "    \n",
    "    # creates df to store acc data\n",
    "    acc_df = pd.DataFrame()\n",
    "    index = task_df['worker_id'].unique()\n",
    "    acc_df = pd.DataFrame(index = index, columns = ['acc'])\n",
    "    \n",
    "    # iterated through every subject to obtain individual acc/ommission rate\n",
    "    for subj in task_df.worker_id.unique():\n",
    "    \n",
    "        subj_df = task_df.loc[task_df.worker_id == subj,].copy()\n",
    "        \n",
    "        if subj_df.empty:\n",
    "            print(f\"subj_df is empty for subject {subj}. Skipping...\")\n",
    "            continue\n",
    "    \n",
    "        \n",
    "        subj_df = subj_df.reset_index(drop=True)\n",
    "        p_idx = subj_df[subj_df.planning==1].index\n",
    "        f_idx = subj_df[subj_df.trial_id=='feedback'].index\n",
    "        assert len(p_idx)==len(f_idx)\n",
    "        moves = f_idx - p_idx\n",
    "        moves = moves/2\n",
    "        unnecessary_moves = moves - 3\n",
    "                \n",
    "        # overall omission rate doesn't need to be included, so I may remove and see if the code works\n",
    "        unique_values = subj_df['experiment_exp_id'].unique()\n",
    "        if len(unique_values) == 0:\n",
    "            print(f\"No unique values for 'experiment_exp_id' for subject {subj}. Skipping...\")\n",
    "            continue\n",
    "        acc_df.loc[(subj, 'WATT_overall_omission_rate')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "        acc_df.loc[(subj, 'omission_rate')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "        \n",
    "        acc_df.loc[(subj, 'acc')] = np.mean(unnecessary_moves)\n",
    "        \n",
    "        # truncation is not being done on WATT due to the difference in accuracy measures we obtain\n",
    "        # included due to errors flaggin when other functions iterate through the final output - may take out\n",
    "        acc_df.loc[(subj, 'WATT_truncation')] = np.nan\n",
    "        \n",
    "    return acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_acc(task_df):\n",
    "    \"\"\"\n",
    "    Function to calc accuracy for discountFix task\n",
    "    We compute accuracy as the proportion of larger later trials.\n",
    "    We also compute ommision rate for the task data\n",
    "    \n",
    "    Input: \n",
    "    - task_df::pandas dataframe:: beh data for task accross all subjects\n",
    "    \n",
    "    Output:\n",
    "    - acc_df::pandas df of accuracy measures for discountFix\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocessing\n",
    "    task_df['correct'] = np.nan\n",
    "            \n",
    "    task_df.loc[task_df.choice=='larger_later', 'correct'] = 1\n",
    "    task_df.loc[task_df.choice=='smaller_sooner', 'correct'] = 0\n",
    "    \n",
    "    # creates df to store acc data\n",
    "    acc_df = pd.DataFrame()\n",
    "    index = task_df['worker_id'].unique()\n",
    "    acc_df = pd.DataFrame(index = index, columns = ['acc'])\n",
    "    \n",
    "    # iterated through every subject to obtain individual acc/ommission rate\n",
    "    for subj in task_df.worker_id.unique():\n",
    "        acc_df.loc[(subj, 'acc')] = task_df.loc[task_df.worker_id == subj, 'correct'].mean()\n",
    "        \n",
    "        subj_df = task_df.loc[(task_df.loc[:,'worker_id'] == subj)]\n",
    "        \n",
    "        # overall omission rate doesn't need to be included, so I may remove and see if the code works\n",
    "        acc_df.loc[(subj, 'omission_rate')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "        acc_df.loc[(subj, 'discountFix_overall_omission_rate')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "        \n",
    "        # truncation is not being done on discount due to the difference in accuracy measures we obtain\n",
    "        # included due to errors flaggin when other functions iterate through the final output - may take out\n",
    "        if 'discountFix_truncation' not in task_df.columns:\n",
    "            acc_df.loc[(subj, 'discountFix_truncation')] = np.nan\n",
    "        else:\n",
    "            acc_df.loc[(subj, 'discountFix_truncation')] == subj_df['truncation'].unique()[0]\n",
    "        \n",
    "        \n",
    "    return acc_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surveyMedley_acc(task_df):\n",
    "    \"\"\"\n",
    "    Function to calc accuracy for surveyMedley task\n",
    "    We compute freuency of responses across all options.\n",
    "    We also compute ommision rate for the task data\n",
    "    \n",
    "    Input: \n",
    "    - task_df::pandas dataframe:: beh data for task accross all subjects\n",
    "    \n",
    "    Output:\n",
    "    - acc_df::pandas df of accuracy measures for CCTHot\n",
    "    \"\"\"\n",
    "    \n",
    "    # creates df to store acc data\n",
    "    acc_df = pd.DataFrame()\n",
    "    index = task_df['worker_id'].unique()\n",
    "    acc_df = pd.DataFrame(index = index, columns = ['omission_rate', 'surveyMedley_omission_rate_overall'])\n",
    "    \n",
    "    # iterated through every subject to obtain individual acc/ommission rate\n",
    "    for subj in task_df.worker_id.unique():\n",
    "        subj_df = task_df.loc[(task_df.loc[:,'worker_id'] == subj)]\n",
    "        \n",
    "        if subj_df.empty == False: \n",
    "            total = len(subj_df.coded_response)\n",
    "            \n",
    "            one = len(subj_df[subj_df.coded_response == 1]) / total\n",
    "            two = len(subj_df[subj_df.coded_response == 2]) / total\n",
    "            three = len(subj_df[subj_df.coded_response == 3]) / total\n",
    "            four = len(subj_df[subj_df.coded_response == 4]) / total\n",
    "            five = len(subj_df[subj_df.coded_response == 5]) / total\n",
    "            \n",
    "        else:\n",
    "            one = 0\n",
    "            two = 0\n",
    "            three = 0 \n",
    "            four = 0\n",
    "            five = 0\n",
    "        \n",
    "        \n",
    "        # overall omission rate doesn't need to be included, so I may remove and see if the code works\n",
    "        acc_df.loc[(subj, 'omission_rate')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "        acc_df.loc[(subj, 'surveyMedley_omission_rate_overall')] = omission_rate(subj_df, subj_df['experiment_exp_id'].unique()[0])\n",
    "         \n",
    "        acc_df.loc[(subj, '1')] = one\n",
    "        acc_df.loc[(subj, '2')] = two\n",
    "        acc_df.loc[(subj, '3')] = three\n",
    "        acc_df.loc[(subj, '4')] = four\n",
    "        acc_df.loc[(subj, '5')] = five\n",
    "        \n",
    "        # truncation is not being done on surveyMedley due to the difference in accuracy measures we obtain\n",
    "        # included due to errors flaggin when other functions iterate through the final output - may take out\n",
    "        acc_df.loc[(subj, 'surveyMedley_truncation')] = np.nan\n",
    "        \n",
    "    return acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_highlight(data_df, outlier_threshold = 3):\n",
    "    \"\"\"\n",
    "    Function to highlight a data frame containing accuracy for task conditions of all subjects\n",
    "    Iterates through df cell by cell and highlightes accordingly\n",
    "    Highlighting:\n",
    "            blue: missing data\n",
    "            orange: more than half of the tasks are missing\n",
    "            yellow: outlier\n",
    "            green: recommended to drop, see flag for further details\n",
    "    \n",
    "    Input: \n",
    "    - data_df::pandas dataframe:: acc of task conditions for all subjects\n",
    "    - threshold::int::scalar value representing outlier threshold (standard deviations away from mean)\n",
    "    Output:\n",
    "    - style::pandas dataframe:: a df that contrains highlighting options to use with pandas styler\n",
    "    \"\"\"\n",
    "    \n",
    "    # creating empty df to populate with highlight options\n",
    "    columns = list(data_df)\n",
    "    style = data_df.copy()\n",
    "    \n",
    "    # iterates through all columns in df\n",
    "    for i in columns:\n",
    "        label = i.split(\"_\")\n",
    "        task = label[0]\n",
    "        \n",
    "        if ('overall_mean' in data_df.index) or ('overall_std' in data_df.index):\n",
    "            mean = np.mean(data_df[i][2:])\n",
    "            std = np.std(data_df[i][2:])\n",
    "        else:\n",
    "            mean = np.mean(data_df[i])\n",
    "            std = np.std(data_df[i])\n",
    "        \n",
    "        # iterates through each individual cell within colums and checks if its an outlier or in drop dictionary\n",
    "        # if subject is in the dropped dictionary for that task it colors accordingly\n",
    "        for index, row in data_df[[i]].iterrows():\n",
    "            if index not in ['overall_std', 'overall_mean']:\n",
    "                if index in dropped_subjects.keys():\n",
    "                    if task in dropped_subjects[index].keys():\n",
    "                        if 'missing_beh_data' in dropped_subjects[index][task]:\n",
    "                            style.loc[index, i] = \"background-color: blue\"\n",
    "                        elif 'missing_more_than_half_the_tasks' in dropped_subjects[index][task]:\n",
    "                            style.loc[index, i] = \"background-color: orange\"\n",
    "                        else:\n",
    "                            style.loc[index, i] = \"background-color: green\"\n",
    "                    elif (abs((row[0]-mean)/std) > outlier_threshold) & (i not in [f'{task}_truncation',f'{task}_overall_omission_rate']):\n",
    "                        style.loc[index, i] = \"background-color: yellow\"\n",
    "                    else:\n",
    "                        style.loc[index, i] = \"\"\n",
    "                elif (abs((row[0]-mean)/std) > outlier_threshold) & (i not in [f'{task}_truncation',f'{task}_overall_omission_rate']):\n",
    "                    style.loc[index, i] = \"background-color: yellow\"\n",
    "                else:\n",
    "                    style.loc[index, i] = \"\"\n",
    "            else:\n",
    "                style.loc[index, i] = \"\"\n",
    "        \n",
    "    return style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(data_df, outlier_threshold = 3):\n",
    "    \"\"\"\n",
    "    Function to obtain outliers in the acc data\n",
    "    Iterates through df cell by cell and obtains outliers\n",
    "    \n",
    "    Input: \n",
    "    - data_df::pandas dataframe:: acc of task conditions for all subjects\n",
    "    - threshold::int::scalar value representing outlier threshold (standard deviations away from mean)\n",
    "    Output:\n",
    "    - outliers::dictionary:: a dictionary that contains the subject and corresponding columns if they are outliers (pass the chosen threshold)\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = list(data_df)\n",
    "    outliers = {}\n",
    "    for column in columns:\n",
    "        label = column.split(\"_\")\n",
    "        task = label[0]\n",
    "        \n",
    "        # obtains mean and st.d for column across all subjects for flagging\n",
    "        mean = np.mean(data_df[column])\n",
    "        std = np.std(data_df[column])\n",
    "        \n",
    "        # iterates throuch each column comparing cells z score to threshold\n",
    "        # appends outliers to dict / doesnt check overall std, truncation, or overall omission\n",
    "        for index, row in data_df[[column]].iterrows():\n",
    "            if (abs((row[0]-mean)/std) > outlier_threshold)  & (index != 'overall_std') & (column not in [f'{task}_truncation', f'{task}_overall_omission_rate']):\n",
    "                if index in outliers.keys():\n",
    "                    outliers[index].append(column)\n",
    "                else:\n",
    "                    outliers[index] = [column]\n",
    "        \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncation_check(df, task, trial_threshold = 3, omission_threshold = .50):\n",
    "    \"\"\"\n",
    "    Function to truncate data \n",
    "    - if there are more than 3 ommisions in a row then all the data after those three omissions will be put into another df.\n",
    "    - if the omission rate of that data is > .5 we truncate the data\n",
    "    - otherwise, the data is left alone\n",
    "    \n",
    "    Input: \n",
    "    - df::pandas dataframe:: beh data for a single subject\n",
    "    - task::str::name of task the data belongs to\n",
    "    - trial_threshold::int::scaler of the number of omissions in a row that will signal truncation\n",
    "    - omission_threshold::float::precentage of omission that we look for to flag truncation\n",
    "    Output:\n",
    "    - df::pandas dataframe:: untouched data, without any trials dropped - was not truncated\n",
    "    - truncated_df::pandas dataframe:: data after truncation\n",
    "    - truncation_rate::float:: percentage of trials that were dropped\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = list(df)\n",
    "    \n",
    "    # list of responses associated with each task\n",
    "    if task == 'stopSignal':\n",
    "        go_trials = df[df.trial_type == 'go']\n",
    "        resp = go_trials['key_press']\n",
    "    elif task == 'motorSelectiveStop':\n",
    "        go_trials = df[(df.trial_type != 'crit_stop_success') & (df.trial_type != 'crit_stop_failure')]\n",
    "        resp = go_trials['key_press']\n",
    "    else:\n",
    "        resp = df['key_press']\n",
    "    \n",
    "    # iterated through every response checking for omissions\n",
    "    count = 0\n",
    "    for i, row in enumerate(resp):\n",
    "        \n",
    "        # ommisions are either -1 or nan\n",
    "        if row == -1:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        \n",
    "        # if we find a series of omissions (n=trial_threshold) we inspect the data\n",
    "        if count == trial_threshold:\n",
    "            index = (i+1) - trial_threshold\n",
    "            \n",
    "            # obtains all the data starting from the omissions and obtains a ref df then obtains their omission rate\n",
    "            if task == 'stopSignal':\n",
    "                ref_df = go_trials[index:]\n",
    "                omission_rate = len(ref_df[ref_df.key_press == -1]) / len(ref_df[ref_df.trial_type == 'go'])\n",
    "                \n",
    "            elif task == 'motorSelectiveStop':\n",
    "                ref_df = df[index:]\n",
    "                omission_rate = len(ref_df[ref_df.key_press == -1]) / len(ref_df)\n",
    "                \n",
    "            else:\n",
    "                ref_df = df[index:]\n",
    "                omission_rate = len(ref_df[ref_df.key_press == -1]) / len(ref_df.key_press)\n",
    "                   \n",
    "            # if the ommision rate of the ref df is more than half we truncate\n",
    "            if omission_rate > omission_threshold:\n",
    "                truncated_df = df[:index]\n",
    "                truncation_rate = len(ref_df)/len(df)\n",
    "                \n",
    "                return truncated_df, truncation_rate\n",
    "            \n",
    "            elif len(ref_df)/len(resp) == 1:\n",
    "                    lst = list(df.key_press)\n",
    "                    cnt = 0\n",
    "                    while lst[cnt] == -1:\n",
    "                        cnt += 1\n",
    "                    \n",
    "                    truncated_df = df[cnt:]\n",
    "                    truncation_rate = 1 - (len(truncated_df)/len(df))\n",
    "                    return truncated_df, truncation_rate\n",
    "            \n",
    "            print(f'failed to truncate omission_rate was less than half of the df: {omission_rate}')\n",
    "            truncation_rate = 0\n",
    "            return df, truncation_rate\n",
    "        \n",
    "    print(f'failed to truncate no sequence of 3 ommisions found.')\n",
    "    truncation_rate = 0\n",
    "    return df, truncation_rate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_analysis(avg_info, data, outlier_threshold = 3, trial_threshold = 3, omission_threshold = .50):\n",
    "    \"\"\"\n",
    "    Function to analyze the outliers that were previously flagged\n",
    "    We take all the flagged outliers and check their omission for possible truncation\n",
    "    \n",
    "    Input: \n",
    "    - avg_info::pandas dataframe:: contains the acc measures for each task condition of interest for all subjects\n",
    "    - data::dict:: dictionary of df's containing all the beh data for all tasks \n",
    "    - threshold::int::threshold to be passed through to the outlier function\n",
    "    Output:\n",
    "    - data::dict:: dictionary of updated beh data for all tasks (updated with truncation)\n",
    "    - nontruncated_outlier::dict:: contains subjects that have not been truncated because they are missing too many trials\n",
    "    \"\"\"\n",
    "    \n",
    "    # obtains outliers\n",
    "    outliers = outlier(avg_info, outlier_threshold)\n",
    "    nontruncated_outlier = {}\n",
    "    \n",
    "    # obtains all the unique tasks we have had outliers for\n",
    "    for subject in outliers.keys():\n",
    "        tasks = []\n",
    "        for flag in outliers[subject]:\n",
    "            label = flag.split(\"_\")\n",
    "            task = label[0]\n",
    "            condition = '_'.join(label[1:-1])\n",
    "            tasks.append(task)\n",
    "\n",
    "        unique_tasks = list(set(tasks))\n",
    "        \n",
    "        # iterates through each outlier\n",
    "        for task in unique_tasks:\n",
    "            \n",
    "            # obtains subject data\n",
    "            subj_df = data[task].loc[(data[task].loc[:,'worker_id'] == subject)]\n",
    "            \n",
    "            # adds truncation and overall omission columns to populate if needed to\n",
    "            if 'truncation' not in data[task].columns:\n",
    "                data[task]['truncation'] = np.nan\n",
    "            \n",
    "            if 'overall_omission_rate' not in data[task].columns:\n",
    "                data[task]['overall_omission_rate'] = np.nan\n",
    "            \n",
    "            # obtains omission rate \n",
    "            non_response_rate = omission_rate(subj_df, task)\n",
    "            \n",
    "            # checks if omissions are to extensive for truncation wihtin outliers\n",
    "            if (non_response_rate < .50) & (task not in ['WATT', 'surveyMedley']):\n",
    "                print(f'subject: {subject} had nonresponses truncated for {task}: {non_response_rate}')\n",
    "                \n",
    "                # checks to see if data can be truncated\n",
    "                response_df, truncation_rate = truncation_check(subj_df, task, trial_threshold, omission_threshold)\n",
    "                \n",
    "                if truncation_rate == 1:\n",
    "                    if subject not in nontruncated_outlier.keys():\n",
    "                        nontruncated_outlier[subject] = [task]\n",
    "                    else:\n",
    "                        nontruncated_outlier[subject].append(task)\n",
    "                \n",
    "                # obtains omission rate before truncation - overall omission\n",
    "                response_df['overall_omission_rate'] = omission_rate(subj_df, task)\n",
    "                \n",
    "                response_df['truncation'] = truncation_rate\n",
    "                data[task] = data[task][data[task]['worker_id'] != subject]\n",
    "                data[task] = data[task].append(response_df)\n",
    "                \n",
    "            \n",
    "            # of omission rate is initially over half, they are dropped - not enough data\n",
    "            elif (non_response_rate > .50):\n",
    "                print(f'subject: {subject} had a nonresponse rate of {non_response_rate} for {task}')\n",
    "                \n",
    "                subj_df['overall_omission_rate'] = omission_rate(subj_df, task)\n",
    "                \n",
    "                \n",
    "                subj_df['truncation'] = 0\n",
    "                data[task] = data[task][data[task]['worker_id'] != subject]\n",
    "                data[task] = data[task].append(subj_df)\n",
    "                \n",
    "                if subject not in nontruncated_outlier.keys():\n",
    "                    nontruncated_outlier[subject] = [task]\n",
    "                else:\n",
    "                    nontruncated_outlier[subject].append(task)\n",
    "                \n",
    "    return data, nontruncated_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_analysis(data, non_truncated, dropped = dropped_subjects, tasks = tasks):\n",
    "    \"\"\"\n",
    "    Function to analyze the final acc df after analyzing outliers. The final acc df contains all acc measures for all subjects .\n",
    "    Obtains a dictionary of dropped subjects and flags for why they are recommended to be dropped \n",
    "    We iterate through different conditions, flagging for the following:\n",
    "        \n",
    "        'greater_than_50_percent_nonresponse_rate': the subject is missing more than half the data due to omissions\n",
    "        'less_than_25_percent_stop_sucess_rate': The subjects stop success rate is greater than 75% (stop/motor)\n",
    "        'more_than_75_percent_stop_sucess_rate': The subjects stop success rate is less than 25% (stop/motor)\n",
    "        'missing_beh_data': subject is missing the beh data for that task\n",
    "        'truncated_more_than_half_data': subject had data truncated, and the data that was truncated accounted for more than 50% of their data\n",
    "        'missing_more_than_half_the_tasks': subject flagged more than 4 tasks for reasons listed above\n",
    "    \n",
    "    Input: \n",
    "    - data::pandas dataframe:: contains the acc measures for each task condition of interest for all subjects\n",
    "    - dropped::dict:: dictionary of dictionares containing subjects - tasks - flags for why tasks are recommened to be dropped\n",
    "    - tasks::list:: tasks we are looking at\n",
    "    Output:\n",
    "    - data::pandas dataframe:: contains each subject acc accross all task conditions of interest\n",
    "    - dropped::dict:: dictionary of dictionares containing subjects - tasks - flags for why tasks are recommened to be dropped\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = list(data)\n",
    "    stop_success = ['motorSelectiveStop_stop_success_rate', 'stopSignal_stop_success_rate']\n",
    "    motor_as_simple_stop_flag = ['motorSelectiveStop_noncrit_signal_omission']\n",
    "    \n",
    "    # iterated through the motor stop non crit signal flagging for acc < .4 [cutoff chosen by patrick]\n",
    "    for column in motor_as_simple_stop_flag:\n",
    "        label = column.split(\"_\")\n",
    "        task = label[0]\n",
    "        \n",
    "        for index, row in data[[column]].iterrows():\n",
    "            if index != 'overall_std': \n",
    "                if (row[0] > .347):\n",
    "                    print(f'{index} dropped for having a noncrit signal omission rate of {row[0]}')\n",
    "                    if index not in dropped.keys():\n",
    "                        if row[0] > .347:\n",
    "                            dropped[index] = {task: ['greater_than_35_precent_om_noncrit_signal_motorstop']}\n",
    "                    else:\n",
    "                        if task not in dropped[index].keys():\n",
    "                            if row[0] > .347:\n",
    "                                dropped[index][task] = ['greater_than_35_precent_om_noncrit_signal_motorstop']\n",
    "                        else:\n",
    "                            if row[0] > .347:\n",
    "                                dropped[index][task].append('greater_than_35_precent_om_noncrit_signal_motorstop')\n",
    "        \n",
    "    \n",
    "    # iterates through the non_truncated dict to see who has an ommision rate > .5 before truncation\n",
    "    for sub, i in non_truncated.items():\n",
    "        for task in i:\n",
    "            if sub not in dropped.keys():\n",
    "                dropped[sub] = {task: ['greater_than_50_percent_nonresponse_rate']}\n",
    "                                    \n",
    "            else:\n",
    "                if task not in dropped[sub].keys():\n",
    "                    dropped[sub][task] = ['greater_than_50_percent_nonresponse_rate']\n",
    "                else:\n",
    "                    dropped[sub][task].append('greater_than_50_percent_nonresponse_rate')\n",
    "    \n",
    "    # iterates through data to flag subjects with stop success < .25 or > .75\n",
    "    for column in stop_success:\n",
    "        label = column.split(\"_\")\n",
    "        task = label[0]\n",
    "\n",
    "        for index, row in data[[column]].iterrows():\n",
    "            if index != 'overall_std': \n",
    "                if (row[0] < .25) or (row[0] > .75):\n",
    "                    print(f'{index} dropped for having a stop success of {row[0]}')\n",
    "                    if index not in dropped.keys():\n",
    "                        if row[0] < .25:\n",
    "                            dropped[index] = {task: ['less_than_25_percent_stop_sucess_rate']}\n",
    "                        elif row[0] > .75:\n",
    "                            dropped[index] = {task: ['more_than_75_percent_stop_sucess_rate']}\n",
    "                    else:\n",
    "                        if task not in dropped[index].keys():\n",
    "                            if row[0] < .25:\n",
    "                                dropped[index][task] = ['less_than_25_percent_stop_sucess_rate']\n",
    "                            elif row[0] > .75:\n",
    "                                dropped[index][task] = ['more_than_75_percent_stop_sucess_rate']\n",
    "                        else:\n",
    "                            if row[0] < .25:\n",
    "                                dropped[index][task].append('less_than_25_percent_stop_sucess_rate')\n",
    "                            elif row[0] > .75:\n",
    "                                dropped[index][task].append('more_than_75_percent_stop_sucess_rate')\n",
    "    \n",
    "    # iterates through data to flag all tasks with missing data \n",
    "    for column in columns:\n",
    "\n",
    "        label = column.split(\"_\")\n",
    "        task = label[0]\n",
    "\n",
    "        for index, row in data[[column]].iterrows():\n",
    "            if (np.isnan(row[0])) & (column not in [f'{task}_truncation', f'{task}_overall_omission_rate', f'{task}_omission_rate']):\n",
    "                if index in dropped.keys():\n",
    "                    if task not in dropped[index].keys():\n",
    "                        dropped[index][task] = ['missing_beh_data']\n",
    "                    else:\n",
    "                        if 'missing_beh_data' not in dropped[index][task]:\n",
    "                            dropped[index][task].append('missing_beh_data')\n",
    "                else:\n",
    "                    dropped[index] = {task: ['missing_beh_data']}\n",
    "    \n",
    "    # iterates through data to flag any subjects that had more than half of their data truncated\n",
    "    for task in tasks:\n",
    "        for index, row in data[[f'{task}_truncation']].iterrows():\n",
    "            if row[0] > .5:\n",
    "                if index in dropped.keys():\n",
    "                    if task not in dropped[index].keys():\n",
    "                        dropped[index][task] = ['truncated_more_than_half_data']\n",
    "                    else:\n",
    "                        if 'truncated_more_than_half_data' not in dropped[index][task]:\n",
    "                            dropped[index][task].append('truncated_more_than_half_data')\n",
    "                else:\n",
    "                    dropped[index] = {task: ['truncated_more_than_half_data']}\n",
    "            \n",
    "    for subject in dropped.keys():\n",
    "        flagged = len(dropped[subject])\n",
    "        if flagged >= 5:\n",
    "            for task in tasks:\n",
    "                if task not in dropped[subject].keys():\n",
    "                    dropped[subject][task] = ['missing_more_than_half_the_tasks']\n",
    "                else:\n",
    "                    if 'missing_more_than_half_the_tasks' not in dropped[subject][task]:\n",
    "                        dropped[subject][task].append('missing_more_than_half_the_tasks')\n",
    "                    \n",
    "\n",
    "    return data, dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data, rt_flag = False, acc_flag = False, overall_metrics = True):\n",
    "    \"\"\"\n",
    "    Function to format the output df appropriatley. This function takes the acc or rt data obtains in the summary functions\n",
    "    and then puts them all into one complete df containing all the subjects and all the desired condition columns.\n",
    "    \n",
    "    Input: \n",
    "    - data::dict:: contains the acc measures for each task condition of interest for all subjects\n",
    "    - rt_flag::bool:: flag if you want rt measures for all tasks\n",
    "    - acc_flag::bool:: flag if you want acc measures for all tasks\n",
    "    - overall_metrics::bool:: flag if you want overall mean and st.d for task conditions(columns of data) - avg across all subjects\n",
    "    Output:\n",
    "    - df::pandas dataframe:: contains rt/acc measures (depending on flag) for all subjects and task conditions of interest\n",
    "    - Final_df::pandas dataframe:: contains rt/acc measures (depending on flag) for all subjects including overall metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # obtains rt metrics\n",
    "    if rt_flag == True:\n",
    "\n",
    "        df = None\n",
    "        # iterates through all tasks and replaces certain columns to be more descriptive\n",
    "        for key in data.keys():\n",
    "            task_df = pd.DataFrame.from_dict(data[key])\n",
    "            if key == 'WATT':\n",
    "                task_df = task_df.rename(columns = {'PA_with_intermediate_rt':'PA_with_rt'})\n",
    "                task_df = task_df.rename(columns = {'PA_without_intermediate_rt':'PA_without_rt'})\n",
    "            elif key == 'CCTHot':\n",
    "                task_df = task_df.rename(columns = {'poldrack-single-stim_rt':'overall_rt'})\n",
    "\n",
    "            task_df.columns = [f'{key}_'+col for col in task_df.columns]\n",
    "            if df is None:\n",
    "                df = task_df\n",
    "            else:\n",
    "                df = pd.concat([df, task_df], axis=1, sort=False)\n",
    "\n",
    "        df = df.sort_index()\n",
    "    \n",
    "    # obtains acc metrics\n",
    "    if acc_flag == True:\n",
    "            \n",
    "        df = None\n",
    "        \n",
    "        # iterates through all tasks and replaces certain columns to be more descriptive\n",
    "        for key in data.keys():\n",
    "            task_df = pd.DataFrame.from_dict(data[key])\n",
    "            if key == 'WATT':\n",
    "                task_df = task_df.rename(columns = {'acc':'WATT_extraMoves'})\n",
    "                task_df = task_df.rename(columns = {'omission_rate':'WATT_omission_rate'})\n",
    "            elif key == 'CCTHot':\n",
    "                task_df = task_df.rename(columns = {'acc':'CCTHot_proportionRewardedTrials'})\n",
    "                task_df = task_df.rename(columns = {'omission_rate':'CCTHot_omission_rate'})\n",
    "            elif key == 'discountFix':\n",
    "                task_df = task_df.rename(columns = {'acc':'discountFix_largerlaterpercentage'})\n",
    "                task_df = task_df.rename(columns = {'omission_rate':'discountFix_omission_rate'})\n",
    "            elif key == 'surveyMedley':\n",
    "                task_df = task_df.rename(columns = {'omission_rate':'surveyMedley_omission_rate'})\n",
    "                task_df = task_df.rename(columns = {'1':'surveyMedley_1_option_rate'})\n",
    "                task_df = task_df.rename(columns = {'2':'surveyMedley_2_option_rate'})\n",
    "                task_df = task_df.rename(columns = {'3':'surveyMedley_3_option_rate'})\n",
    "                task_df = task_df.rename(columns = {'4':'surveyMedley_4_option_rate'})\n",
    "                task_df = task_df.rename(columns = {'5':'surveyMedley_5_option_rate'})\n",
    "            else:\n",
    "                task_df.columns = [f'{key}_'+col for col in task_df.columns]\n",
    "            if df is None:\n",
    "                df = task_df\n",
    "            else:\n",
    "                df = pd.concat([df, task_df], axis=1, sort=False)\n",
    "        \n",
    "        df = df.sort_index()\n",
    "\n",
    "        # flip WATT, essentially from errors to acc\n",
    "        if 'WATT_extraMoves' in df.columns:\n",
    "            df['WATT_extraMoves'] = df['WATT_extraMoves'].max() - df['WATT_extraMoves']\n",
    "    \n",
    "    # obtains overall avg/std for every column\n",
    "    if overall_metrics == True:\n",
    "        \n",
    "        # getting average rt accross all subjects for each condition\n",
    "        column_avg = df.mean(axis = 0).tolist()\n",
    "        avg = pd.DataFrame([column_avg], columns=df.columns.tolist())\n",
    "        avg.index = ['overall_mean']\n",
    "\n",
    "        # getting std across all subjects for each condition\n",
    "        column_std = df.std(axis = 0).tolist()\n",
    "        std = pd.DataFrame([column_std], columns=df.columns.tolist())\n",
    "        std.index = ['overall_std']\n",
    "\n",
    "        overall_metrics = pd.concat([avg, std])\n",
    "        Final_df = pd.concat([overall_metrics, df])\n",
    "        return Final_df\n",
    "    \n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all task data into dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alldata_dict = {}\n",
    "for task in tasks: \n",
    "    task_df = pd.DataFrame()\n",
    "    for file in file_dict[task]: \n",
    "        df = pd.DataFrame()\n",
    "        df = pd.read_csv(file, sep='\\t')\n",
    "        df.dropna(subset=['worker_id'], inplace=True)\n",
    "        task_df = task_df.append(df, sort=True)   \n",
    "    \n",
    "    #create ANT conditions\n",
    "    if task == 'ANT':\n",
    "        task_df['trial_type'] = task_df['cue'] + '_' + task_df['flanker_type']\n",
    "        \n",
    "    if task in ['motorSelectiveStop', 'stopSignal',]:\n",
    "        task_df['correct'] = task_df.apply(lambda row: 1 if row['key_press'] == row['correct_response'] else 0, axis=1)\n",
    "\n",
    "        \n",
    "    alldata_dict[task] = task_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT Summary stats for each condition of each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_dict = {\n",
    "    'twoByTwo': alldata_dict['twoByTwo']['switch_type'].unique().tolist(),\n",
    "    'stopSignal': ['go', 'stop_failure'],\n",
    "    'motorSelectiveStop': ['noncrit_signal', 'noncrit_nosignal', 'crit_stop_failure', 'crit_go'],\n",
    "    'ANT': alldata_dict['ANT']['flanker_type'].unique().tolist(),\n",
    "    'discountFix': ['smaller_sooner', 'larger_later'],\n",
    "    'stroop': alldata_dict['stroop']['trial_type'].unique().tolist(),\n",
    "    'WATT': alldata_dict['WATT']['condition'].unique().tolist(),\n",
    "}\n",
    "column_dict = {\n",
    "    'twoByTwo': 'switch_type',\n",
    "    'ANT': 'flanker_type', \n",
    "    'discountFix': 'choice',\n",
    "    'WATT': 'condition',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping CCTHot due to missing 'trial_type' column.\n"
     ]
    }
   ],
   "source": [
    "rt_dict = {}\n",
    "for task in tasks:         \n",
    "    if task != 'surveyMedley':\n",
    "        if task in condition_dict:\n",
    "            conditions = condition_dict[task]\n",
    "            cond_col = column_dict.get(task, 'trial_type')\n",
    "        else:\n",
    "            try:\n",
    "                conditions = alldata_dict[task]['trial_type'].unique().tolist()\n",
    "                cond_col = 'trial_type'\n",
    "            except KeyError:\n",
    "                print(f\"Skipping {task} due to missing 'trial_type' column.\")\n",
    "                continue\n",
    "                \n",
    "        rt_dict[task] = rt_summary(alldata_dict[task], conditions, cond_col)\n",
    "    \n",
    "FINAL_task_rt_df = format_data(rt_dict, rt_flag = True, acc_flag = False, overall_metrics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANT_spatial_incongruent_rt</th>\n",
       "      <th>ANT_double_incongruent_rt</th>\n",
       "      <th>ANT_spatial_congruent_rt</th>\n",
       "      <th>ANT_double_congruent_rt</th>\n",
       "      <th>discountFix_smaller_sooner_rt</th>\n",
       "      <th>discountFix_larger_later_rt</th>\n",
       "      <th>DPX_AX_rt</th>\n",
       "      <th>DPX_AY_rt</th>\n",
       "      <th>DPX_BX_rt</th>\n",
       "      <th>DPX_BY_rt</th>\n",
       "      <th>motorSelectiveStop_noncrit_signal_rt</th>\n",
       "      <th>motorSelectiveStop_noncrit_nosignal_rt</th>\n",
       "      <th>motorSelectiveStop_crit_stop_failure_rt</th>\n",
       "      <th>motorSelectiveStop_crit_go_rt</th>\n",
       "      <th>motorSelectiveStop_mean_SSD</th>\n",
       "      <th>stopSignal_go_rt</th>\n",
       "      <th>stopSignal_stop_failure_rt</th>\n",
       "      <th>twoByTwo_task_switch_rt</th>\n",
       "      <th>twoByTwo_cue_stay_rt</th>\n",
       "      <th>twoByTwo_cue_switch_rt</th>\n",
       "      <th>stroop_congruent_rt</th>\n",
       "      <th>stroop_incongruent_rt</th>\n",
       "      <th>WATT_nan_rt</th>\n",
       "      <th>WATT_UA_with_intermediate_rt</th>\n",
       "      <th>WATT_UA_without_intermediate_rt</th>\n",
       "      <th>WATT_PA_with_rt</th>\n",
       "      <th>WATT_PA_without_rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall_mean</th>\n",
       "      <td>0.588174</td>\n",
       "      <td>0.637917</td>\n",
       "      <td>0.576917</td>\n",
       "      <td>0.604836</td>\n",
       "      <td>1.52648</td>\n",
       "      <td>1.519464</td>\n",
       "      <td>0.53189</td>\n",
       "      <td>0.639819</td>\n",
       "      <td>0.452191</td>\n",
       "      <td>0.465915</td>\n",
       "      <td>0.628662</td>\n",
       "      <td>0.598228</td>\n",
       "      <td>0.644335</td>\n",
       "      <td>0.712547</td>\n",
       "      <td>0.350570</td>\n",
       "      <td>0.666337</td>\n",
       "      <td>0.590753</td>\n",
       "      <td>0.77732</td>\n",
       "      <td>0.711777</td>\n",
       "      <td>0.756088</td>\n",
       "      <td>0.642193</td>\n",
       "      <td>0.740818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.65841</td>\n",
       "      <td>3.557401</td>\n",
       "      <td>3.774927</td>\n",
       "      <td>3.289199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall_std</th>\n",
       "      <td>0.272099</td>\n",
       "      <td>0.111176</td>\n",
       "      <td>0.103228</td>\n",
       "      <td>0.111112</td>\n",
       "      <td>0.395192</td>\n",
       "      <td>0.422202</td>\n",
       "      <td>0.114112</td>\n",
       "      <td>0.116572</td>\n",
       "      <td>0.138351</td>\n",
       "      <td>0.141108</td>\n",
       "      <td>0.13123</td>\n",
       "      <td>0.126332</td>\n",
       "      <td>0.131609</td>\n",
       "      <td>0.170806</td>\n",
       "      <td>0.185483</td>\n",
       "      <td>0.162752</td>\n",
       "      <td>0.115952</td>\n",
       "      <td>0.133666</td>\n",
       "      <td>0.115116</td>\n",
       "      <td>0.13684</td>\n",
       "      <td>0.106519</td>\n",
       "      <td>0.14089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.786598</td>\n",
       "      <td>1.599828</td>\n",
       "      <td>1.635261</td>\n",
       "      <td>1.669714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s061</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.860722</td>\n",
       "      <td>1.70206</td>\n",
       "      <td>0.595705</td>\n",
       "      <td>0.73125</td>\n",
       "      <td>0.534125</td>\n",
       "      <td>0.500792</td>\n",
       "      <td>0.602735</td>\n",
       "      <td>0.575933</td>\n",
       "      <td>0.6106</td>\n",
       "      <td>0.606605</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.734613</td>\n",
       "      <td>0.739037</td>\n",
       "      <td>0.859561</td>\n",
       "      <td>0.803879</td>\n",
       "      <td>0.893098</td>\n",
       "      <td>0.650292</td>\n",
       "      <td>0.851125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.7735</td>\n",
       "      <td>2.7765</td>\n",
       "      <td>2.937583</td>\n",
       "      <td>2.506292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s130</th>\n",
       "      <td>0.500563</td>\n",
       "      <td>0.579719</td>\n",
       "      <td>0.455771</td>\n",
       "      <td>0.569724</td>\n",
       "      <td>1.621867</td>\n",
       "      <td>1.580556</td>\n",
       "      <td>0.96157</td>\n",
       "      <td>0.985167</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>0.862375</td>\n",
       "      <td>0.540102</td>\n",
       "      <td>0.527079</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.699173</td>\n",
       "      <td>0.610292</td>\n",
       "      <td>0.823151</td>\n",
       "      <td>0.691651</td>\n",
       "      <td>0.688333</td>\n",
       "      <td>0.648255</td>\n",
       "      <td>0.761458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.2565</td>\n",
       "      <td>3.6305</td>\n",
       "      <td>4.473682</td>\n",
       "      <td>3.496083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s144</th>\n",
       "      <td>0.375938</td>\n",
       "      <td>0.431437</td>\n",
       "      <td>0.3614</td>\n",
       "      <td>0.414586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.474013</td>\n",
       "      <td>0.44872</td>\n",
       "      <td>0.499418</td>\n",
       "      <td>0.500413</td>\n",
       "      <td>0.506333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.115</td>\n",
       "      <td>1.6845</td>\n",
       "      <td>1.85575</td>\n",
       "      <td>1.591083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s646</th>\n",
       "      <td>0.580063</td>\n",
       "      <td>0.573906</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.555414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.729814</td>\n",
       "      <td>0.657167</td>\n",
       "      <td>0.748844</td>\n",
       "      <td>0.745492</td>\n",
       "      <td>0.814194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.1665</td>\n",
       "      <td>2.4855</td>\n",
       "      <td>2.921542</td>\n",
       "      <td>1.931125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s647</th>\n",
       "      <td>0.747645</td>\n",
       "      <td>0.669156</td>\n",
       "      <td>0.628343</td>\n",
       "      <td>0.669828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.654643</td>\n",
       "      <td>0.794625</td>\n",
       "      <td>0.435792</td>\n",
       "      <td>0.563583</td>\n",
       "      <td>0.602245</td>\n",
       "      <td>0.56164</td>\n",
       "      <td>0.54212</td>\n",
       "      <td>0.660171</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.524608</td>\n",
       "      <td>0.487087</td>\n",
       "      <td>0.728089</td>\n",
       "      <td>0.660245</td>\n",
       "      <td>0.728794</td>\n",
       "      <td>0.5055</td>\n",
       "      <td>0.578937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.1605</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.194667</td>\n",
       "      <td>1.37525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s648</th>\n",
       "      <td>0.549719</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.520971</td>\n",
       "      <td>0.522517</td>\n",
       "      <td>1.070545</td>\n",
       "      <td>1.142526</td>\n",
       "      <td>0.6765</td>\n",
       "      <td>0.793625</td>\n",
       "      <td>0.530333</td>\n",
       "      <td>0.501364</td>\n",
       "      <td>0.606327</td>\n",
       "      <td>0.615697</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.919797</td>\n",
       "      <td>0.571200</td>\n",
       "      <td>0.837587</td>\n",
       "      <td>0.685273</td>\n",
       "      <td>1.016537</td>\n",
       "      <td>0.936076</td>\n",
       "      <td>1.05562</td>\n",
       "      <td>0.708083</td>\n",
       "      <td>0.769312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.983</td>\n",
       "      <td>6.057</td>\n",
       "      <td>4.835261</td>\n",
       "      <td>4.502826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s649</th>\n",
       "      <td>0.548438</td>\n",
       "      <td>0.563625</td>\n",
       "      <td>0.530229</td>\n",
       "      <td>0.512552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.722359</td>\n",
       "      <td>0.510795</td>\n",
       "      <td>0.566583</td>\n",
       "      <td>0.38845</td>\n",
       "      <td>0.513391</td>\n",
       "      <td>0.775818</td>\n",
       "      <td>0.773444</td>\n",
       "      <td>0.651952</td>\n",
       "      <td>0.870422</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.735153</td>\n",
       "      <td>0.61736</td>\n",
       "      <td>0.924815</td>\n",
       "      <td>0.819359</td>\n",
       "      <td>0.89558</td>\n",
       "      <td>0.709562</td>\n",
       "      <td>0.757042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.175</td>\n",
       "      <td>2.761</td>\n",
       "      <td>3.124417</td>\n",
       "      <td>2.646083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s650</th>\n",
       "      <td>0.791571</td>\n",
       "      <td>0.9144</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.882143</td>\n",
       "      <td>1.5639</td>\n",
       "      <td>1.630667</td>\n",
       "      <td>0.528814</td>\n",
       "      <td>0.691778</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.745357</td>\n",
       "      <td>0.977583</td>\n",
       "      <td>0.87312</td>\n",
       "      <td>0.763375</td>\n",
       "      <td>0.851519</td>\n",
       "      <td>0.746200</td>\n",
       "      <td>0.858938</td>\n",
       "      <td>0.6894</td>\n",
       "      <td>1.207965</td>\n",
       "      <td>0.907043</td>\n",
       "      <td>1.075297</td>\n",
       "      <td>1.011689</td>\n",
       "      <td>1.23815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0705</td>\n",
       "      <td>5.7055</td>\n",
       "      <td>5.434333</td>\n",
       "      <td>4.112667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ANT_spatial_incongruent_rt ANT_double_incongruent_rt  \\\n",
       "overall_mean  0.588174                   0.637917                   \n",
       "overall_std   0.272099                   0.111176                   \n",
       "s061          NaN                        NaN                        \n",
       "s130          0.500563                   0.579719                   \n",
       "s144          0.375938                   0.431437                   \n",
       "...                ...                        ...                   \n",
       "s646          0.580063                   0.573906                   \n",
       "s647          0.747645                   0.669156                   \n",
       "s648          0.549719                   0.588                      \n",
       "s649          0.548438                   0.563625                   \n",
       "s650          0.791571                   0.9144                     \n",
       "\n",
       "             ANT_spatial_congruent_rt ANT_double_congruent_rt  \\\n",
       "overall_mean  0.576917                 0.604836                 \n",
       "overall_std   0.103228                 0.111112                 \n",
       "s061          NaN                      NaN                      \n",
       "s130          0.455771                 0.569724                 \n",
       "s144          0.3614                   0.414586                 \n",
       "...              ...                        ...                 \n",
       "s646          0.529                    0.555414                 \n",
       "s647          0.628343                 0.669828                 \n",
       "s648          0.520971                 0.522517                 \n",
       "s649          0.530229                 0.512552                 \n",
       "s650          0.7915                   0.882143                 \n",
       "\n",
       "             discountFix_smaller_sooner_rt discountFix_larger_later_rt  \\\n",
       "overall_mean  1.52648                       1.519464                     \n",
       "overall_std   0.395192                      0.422202                     \n",
       "s061          1.860722                      1.70206                      \n",
       "s130          1.621867                      1.580556                     \n",
       "s144          NaN                           NaN                          \n",
       "...           ...                           ...                          \n",
       "s646          NaN                           NaN                          \n",
       "s647          NaN                           NaN                          \n",
       "s648          1.070545                      1.142526                     \n",
       "s649          NaN                           0.722359                     \n",
       "s650          1.5639                        1.630667                     \n",
       "\n",
       "             DPX_AX_rt DPX_AY_rt DPX_BX_rt DPX_BY_rt  \\\n",
       "overall_mean  0.53189   0.639819  0.452191  0.465915   \n",
       "overall_std   0.114112  0.116572  0.138351  0.141108   \n",
       "s061          0.595705  0.73125   0.534125  0.500792   \n",
       "s130          0.96157   0.985167  0.8805    0.862375   \n",
       "s144          NaN       NaN       NaN       NaN        \n",
       "...           ...       ...       ...       ...        \n",
       "s646          NaN       NaN       NaN       NaN        \n",
       "s647          0.654643  0.794625  0.435792  0.563583   \n",
       "s648          0.6765    0.793625  0.530333  0.501364   \n",
       "s649          0.510795  0.566583  0.38845   0.513391   \n",
       "s650          0.528814  0.691778  0.456     0.745357   \n",
       "\n",
       "             motorSelectiveStop_noncrit_signal_rt  \\\n",
       "overall_mean  0.628662                              \n",
       "overall_std   0.13123                               \n",
       "s061          0.602735                              \n",
       "s130          0.540102                              \n",
       "s144          NaN                                   \n",
       "...           ...                                   \n",
       "s646          NaN                                   \n",
       "s647          0.602245                              \n",
       "s648          0.606327                              \n",
       "s649          0.775818                              \n",
       "s650          0.977583                              \n",
       "\n",
       "             motorSelectiveStop_noncrit_nosignal_rt  \\\n",
       "overall_mean  0.598228                                \n",
       "overall_std   0.126332                                \n",
       "s061          0.575933                                \n",
       "s130          0.527079                                \n",
       "s144          NaN                                     \n",
       "...           ...                                     \n",
       "s646          NaN                                     \n",
       "s647          0.56164                                 \n",
       "s648          0.615697                                \n",
       "s649          0.773444                                \n",
       "s650          0.87312                                 \n",
       "\n",
       "             motorSelectiveStop_crit_stop_failure_rt  \\\n",
       "overall_mean  0.644335                                 \n",
       "overall_std   0.131609                                 \n",
       "s061          0.6106                                   \n",
       "s130          0.579                                    \n",
       "s144          NaN                                      \n",
       "...           ...                                      \n",
       "s646          NaN                                      \n",
       "s647          0.54212                                  \n",
       "s648          0.75                                     \n",
       "s649          0.651952                                 \n",
       "s650          0.763375                                 \n",
       "\n",
       "             motorSelectiveStop_crit_go_rt  motorSelectiveStop_mean_SSD  \\\n",
       "overall_mean  0.712547                      0.350570                      \n",
       "overall_std   0.170806                      0.185483                      \n",
       "s061          0.606605                      0.354000                      \n",
       "s130          0.714                         0.445600                      \n",
       "s144          NaN                          NaN                            \n",
       "...           ...                           ..                            \n",
       "s646          NaN                          NaN                            \n",
       "s647          0.660171                      0.322000                      \n",
       "s648          0.919797                      0.571200                      \n",
       "s649          0.870422                      0.576000                      \n",
       "s650          0.851519                      0.746200                      \n",
       "\n",
       "             stopSignal_go_rt stopSignal_stop_failure_rt  \\\n",
       "overall_mean  0.666337         0.590753                    \n",
       "overall_std   0.162752         0.115952                    \n",
       "s061          0.734613         0.739037                    \n",
       "s130          0.699173         0.610292                    \n",
       "s144          0.474013         0.44872                     \n",
       "...                ...             ...                     \n",
       "s646          0.729814         0.657167                    \n",
       "s647          0.524608         0.487087                    \n",
       "s648          0.837587         0.685273                    \n",
       "s649          0.735153         0.61736                     \n",
       "s650          0.858938         0.6894                      \n",
       "\n",
       "             twoByTwo_task_switch_rt twoByTwo_cue_stay_rt  \\\n",
       "overall_mean  0.77732                 0.711777              \n",
       "overall_std   0.133666                0.115116              \n",
       "s061          0.859561                0.803879              \n",
       "s130          0.823151                0.691651              \n",
       "s144          0.499418                0.500413              \n",
       "...                ...                     ...              \n",
       "s646          0.748844                0.745492              \n",
       "s647          0.728089                0.660245              \n",
       "s648          1.016537                0.936076              \n",
       "s649          0.924815                0.819359              \n",
       "s650          1.207965                0.907043              \n",
       "\n",
       "             twoByTwo_cue_switch_rt stroop_congruent_rt stroop_incongruent_rt  \\\n",
       "overall_mean  0.756088               0.642193            0.740818               \n",
       "overall_std   0.13684                0.106519            0.14089                \n",
       "s061          0.893098               0.650292            0.851125               \n",
       "s130          0.688333               0.648255            0.761458               \n",
       "s144          0.506333               NaN                 NaN                    \n",
       "...                ...               ...                 ...                    \n",
       "s646          0.814194               NaN                 NaN                    \n",
       "s647          0.728794               0.5055              0.578937               \n",
       "s648          1.05562                0.708083            0.769312               \n",
       "s649          0.89558                0.709562            0.757042               \n",
       "s650          1.075297               1.011689            1.23815                \n",
       "\n",
       "             WATT_nan_rt WATT_UA_with_intermediate_rt  \\\n",
       "overall_mean  NaN         4.65841                       \n",
       "overall_std   NaN         1.786598                      \n",
       "s061          NaN         4.7735                        \n",
       "s130          NaN         5.2565                        \n",
       "s144          NaN         2.115                         \n",
       "...           ...           ...                         \n",
       "s646          NaN         5.1665                        \n",
       "s647          NaN         3.1605                        \n",
       "s648          NaN         4.983                         \n",
       "s649          NaN         4.175                         \n",
       "s650          NaN         9.0705                        \n",
       "\n",
       "             WATT_UA_without_intermediate_rt WATT_PA_with_rt  \\\n",
       "overall_mean  3.557401                        3.774927         \n",
       "overall_std   1.599828                        1.635261         \n",
       "s061          2.7765                          2.937583         \n",
       "s130          3.6305                          4.473682         \n",
       "s144          1.6845                          1.85575          \n",
       "...              ...                              ...          \n",
       "s646          2.4855                          2.921542         \n",
       "s647          2.89                            1.194667         \n",
       "s648          6.057                           4.835261         \n",
       "s649          2.761                           3.124417         \n",
       "s650          5.7055                          5.434333         \n",
       "\n",
       "             WATT_PA_without_rt  \n",
       "overall_mean  3.289199           \n",
       "overall_std   1.669714           \n",
       "s061          2.506292           \n",
       "s130          3.496083           \n",
       "s144          1.591083           \n",
       "...                ...           \n",
       "s646          1.931125           \n",
       "s647          1.37525            \n",
       "s648          4.502826           \n",
       "s649          2.646083           \n",
       "s650          4.112667           \n",
       "\n",
       "[112 rows x 27 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_task_rt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_task_rt_df.style.apply(outlier_highlight, axis=None).to_excel(\"aim1_raw_rt.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Task Acc / Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cond_dict = {\n",
    "    'twoByTwo': alldata_dict['twoByTwo']['switch_type'].unique().tolist(),\n",
    "    'stopSignal': ['go', 'stop_failure'],\n",
    "    'motorSelectiveStop': ['noncrit_signal', 'noncrit_nosignal', 'crit_stop_failure', 'crit_go'],\n",
    "    'ANT': alldata_dict['ANT']['flanker_type'].unique().tolist(),\n",
    "    'discountFix': ['smaller_sooner', 'larger_later'],\n",
    "    'stroop': alldata_dict['stroop']['trial_type'].unique().tolist(),\n",
    "    'WATT': None,\n",
    "    'CCTHot': None\n",
    "}\n",
    "\n",
    "column_dict = {\n",
    "    'twoByTwo': 'switch_type',\n",
    "    'ANT': 'flanker_type', \n",
    "}\n",
    "\n",
    "acc_funcs = {\n",
    "    'CCTHot': CCTHot_EV_acc,\n",
    "    'discountFix': discount_acc,\n",
    "    'WATT': WATT_acc,\n",
    "    'surveyMedley': surveyMedley_acc,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_acc_dict = {}\n",
    "\n",
    "for task in tasks:\n",
    "    if task in acc_cond_dict:\n",
    "        conditions = acc_cond_dict[task]\n",
    "        cond_col = column_dict.get(task, 'trial_type')\n",
    "    else:\n",
    "        try:\n",
    "            conditions = alldata_dict[task]['trial_type'].unique().tolist()\n",
    "            cond_col = 'trial_type'\n",
    "        except KeyError:\n",
    "            print(f\"Skipping {task} due to missing 'trial_type' column.\")\n",
    "            continue\n",
    "                \n",
    "    if task in acc_funcs:\n",
    "        func = acc_funcs[task]\n",
    "    else:\n",
    "        func = acc_funcs.get(task, lambda x: acc_summary(x, conditions, condition_column = cond_col))\n",
    "    \n",
    "    full_acc_dict[task] = func(alldata_dict[task])\n",
    "    \n",
    "FINAL_task_acc_df = format_data(full_acc_dict, rt_flag = False, acc_flag = True, overall_metrics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_task_acc_df.style.apply(outlier_highlight, axis=None).to_excel(\"aim1_raw_acc.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: s635 had a nonresponse rate of 0.6953125 for ANT\n",
      "subject: s635 had a nonresponse rate of 0.5166666666666667 for twoByTwo\n",
      "subject: s650 had a nonresponse rate of 0.68 for motorSelectiveStop\n",
      "subject: s650 had nonresponses truncated for discountFix: 0.475\n",
      "subject: s650 had a nonresponse rate of 0.5125 for twoByTwo\n",
      "subject: s650 had nonresponses truncated for ANT: 0.328125\n",
      "failed to truncate omission_rate was less than half of the df: 0.4666666666666667\n",
      "subject: s650 had nonresponses truncated for stopSignal: 0.36\n",
      "subject: s650 had a nonresponse rate of 0.625 for DPX\n",
      "subject: s524 had nonresponses truncated for discountFix: 0.3\n",
      "subject: s524 had nonresponses truncated for stroop: 0.17708333333333334\n",
      "failed to truncate omission_rate was less than half of the df: 0.23076923076923078\n",
      "subject: s613 had nonresponses truncated for discountFix: 0.21666666666666667\n",
      "failed to truncate omission_rate was less than half of the df: 0.2857142857142857\n",
      "subject: s613 had a nonresponse rate of 0.653125 for DPX\n",
      "subject: s623 had nonresponses truncated for motorSelectiveStop: 0.32\n",
      "subject: s623 had nonresponses truncated for discountFix: 0.3\n",
      "failed to truncate omission_rate was less than half of the df: 0.3333333333333333\n",
      "subject: s623 had nonresponses truncated for twoByTwo: 0.24583333333333332\n",
      "subject: s623 had a nonresponse rate of 0.671875 for DPX\n",
      "subject: s586 had nonresponses truncated for motorSelectiveStop: 0.025\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s586 had a nonresponse rate of 0.5125 for DPX\n",
      "subject: s638 had a nonresponse rate of 0.525 for DPX\n",
      "subject: s519 had nonresponses truncated for motorSelectiveStop: 0.155\n",
      "failed to truncate omission_rate was less than half of the df: 0.23469387755102042\n",
      "subject: s577 had nonresponses truncated for motorSelectiveStop: 0.145\n",
      "failed to truncate omission_rate was less than half of the df: 0.35\n",
      "subject: s649 had nonresponses truncated for motorSelectiveStop: 0.385\n",
      "failed to truncate omission_rate was less than half of the df: 0.47150259067357514\n",
      "subject: s548 had nonresponses truncated for motorSelectiveStop: 0.0\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s557 had nonresponses truncated for motorSelectiveStop: 0.005\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s636 had nonresponses truncated for motorSelectiveStop: 0.035\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s574 had nonresponses truncated for motorSelectiveStop: 0.055\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s608 had nonresponses truncated for motorSelectiveStop: 0.015\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s608 had nonresponses truncated for stroop: 0.16666666666666666\n",
      "failed to truncate omission_rate was less than half of the df: 0.3333333333333333\n",
      "subject: s619 had nonresponses truncated for motorSelectiveStop: 0.05\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s499 had nonresponses truncated for motorSelectiveStop: 0.08\n",
      "failed to truncate omission_rate was less than half of the df: 0.17699115044247787\n",
      "subject: s499 had nonresponses truncated for stroop: 0.22916666666666666\n",
      "failed to truncate omission_rate was less than half of the df: 0.4\n",
      "subject: s512 had a nonresponse rate of 0.8666666666666667 for stopSignal\n",
      "subject: s556 had nonresponses truncated for stopSignal: 0.0\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s585 had nonresponses truncated for stopSignal: 0.0\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s595 had nonresponses truncated for stopSignal: 0.013333333333333334\n",
      "failed to truncate no sequence of 3 ommisions found.\n",
      "subject: s445 had nonresponses truncated for stroop: 0.21875\n",
      "subject: s570 had nonresponses truncated for stroop: 0.2604166666666667\n",
      "failed to truncate omission_rate was less than half of the df: 0.36666666666666664\n",
      "subject: s589 had nonresponses truncated for CCTHot: 0.45454545454545453\n",
      "failed to truncate no sequence of 3 ommisions found.\n"
     ]
    }
   ],
   "source": [
    "data, non_truncated_subj = outlier_analysis(FINAL_task_acc_df, alldata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_acc_dict = {}\n",
    "for task in data.keys():\n",
    "\n",
    "    if task in acc_cond_dict:\n",
    "        conditions = acc_cond_dict[task]\n",
    "        cond_col = column_dict.get(task, 'trial_type')\n",
    "    else:\n",
    "        try:\n",
    "            conditions = data[task]['trial_type'].unique().tolist()\n",
    "            cond_col = 'trial_type'\n",
    "        except KeyError:\n",
    "            print(f\"Skipping {task} due to missing 'trial_type' column.\")\n",
    "            continue\n",
    "                \n",
    "    if task in acc_funcs:\n",
    "        func = acc_funcs[task]\n",
    "    else:\n",
    "        func = acc_funcs.get(task, lambda x: acc_summary(x, conditions, condition_column = cond_col))\n",
    "    \n",
    "    outlier_acc_dict[task] = func(data[task])\n",
    "\n",
    "outlier_adjusted_aim1_acc = format_data(outlier_acc_dict, rt_flag = False, acc_flag = True, overall_metrics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s519 dropped for having a noncrit signal omission rate of 0.6326530612244898\n",
      "s549 dropped for having a noncrit signal omission rate of 0.42857142857142855\n",
      "s577 dropped for having a noncrit signal omission rate of 0.5918367346938775\n",
      "s650 dropped for having a noncrit signal omission rate of 0.7551020408163265\n",
      "s548 dropped for having a stop success of 0.8\n",
      "s557 dropped for having a stop success of 0.82\n",
      "s574 dropped for having a stop success of 0.24\n",
      "s577 dropped for having a stop success of 0.8\n",
      "s608 dropped for having a stop success of 0.14\n",
      "s650 dropped for having a stop success of 0.84\n",
      "s512 dropped for having a stop success of 0.94\n",
      "s556 dropped for having a stop success of 0.86\n",
      "s585 dropped for having a stop success of 0.8\n"
     ]
    }
   ],
   "source": [
    "df, dropped_subjects = overall_analysis(outlier_adjusted_aim1_acc, non_truncated_subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style.apply(outlier_highlight, axis=None).to_excel(\"outlier_adjusted_aim1_acc_new_events.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropped Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtains df of exclusions for subjects_tasks for jeanette\n",
    "exclusion_criteria = []\n",
    "index = []\n",
    "for sub in dropped_subjects.keys():\n",
    "    for task in dropped_subjects[sub].keys():\n",
    "        index.append(f'{sub}_{task}')\n",
    "        for flag in dropped_subjects[sub][task]:\n",
    "            exclusion_criteria.append(flag)\n",
    "\n",
    "        \n",
    "columns = set(exclusion_criteria)\n",
    "exclusion_df = pd.DataFrame(np.nan, index = index, columns = columns)\n",
    "\n",
    "for sub in dropped_subjects.keys():\n",
    "    for task in dropped_subjects[sub].keys():\n",
    "        exclusion_df.loc[f'{sub}_{task}', dropped_subjects[sub][task]] = 1\n",
    "\n",
    "        \n",
    "exclusion_df = exclusion_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorporating Jeanettes MRIQC Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mriqc_exclusions = pd.read_csv('QA_lev2_notes-mriqc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mriqc_exclusions = mriqc_exclusions.drop(columns=['session', 'notes', 'okay', 'fmriprep check (only done on questionable data)', 'SJ', 'Unnamed: 8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mriqc_exclusions = mriqc_exclusions[mriqc_exclusions['good_on_final_check'] == 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dropped_subjects = copy.deepcopy(dropped_subjects)\n",
    "\n",
    "for ind in mriqc_exclusions.index:\n",
    "    subject = mriqc_exclusions['subject'][ind]\n",
    "    subject = f's{subject}'\n",
    "    task = mriqc_exclusions['task'][ind]\n",
    "    if task == 'WATT3':\n",
    "        task = 'WATT'\n",
    "        \n",
    "    if subject in updated_dropped_subjects.keys():\n",
    "        if task in updated_dropped_subjects[subject].keys():\n",
    "            updated_dropped_subjects[subject][task].append('MRIQC_fail')\n",
    "        else:\n",
    "            updated_dropped_subjects[subject][task] = ['MRIQC_fail']\n",
    "    else:\n",
    "        updated_dropped_subjects[subject] = {task: ['MRIQC_fail']}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in updated_dropped_subjects.keys():\n",
    "    if len(updated_dropped_subjects[sub].keys()) >= 5:\n",
    "        for task in tasks:\n",
    "            if task in updated_dropped_subjects[sub].keys():\n",
    "                if 'missing_more_than_half_the_tasks' not in updated_dropped_subjects[sub][task]:\n",
    "                    updated_dropped_subjects[sub][task].append('missing_more_than_half_the_tasks')\n",
    "            else:\n",
    "                updated_dropped_subjects[sub][task] = ['missing_more_than_half_the_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtains df of exclusions for subjects_tasks for jeanette\n",
    "exclusion_criteria = []\n",
    "index = []\n",
    "for sub in updated_dropped_subjects.keys():\n",
    "    for task in updated_dropped_subjects[sub].keys():\n",
    "        index.append(f'{sub}_{task}')\n",
    "        for flag in updated_dropped_subjects[sub][task]:\n",
    "            exclusion_criteria.append(flag)\n",
    "\n",
    "        \n",
    "columns = set(exclusion_criteria)\n",
    "updated_exclusion_df = pd.DataFrame(np.nan, index = index, columns = columns)\n",
    "\n",
    "for sub in updated_dropped_subjects.keys():\n",
    "    for task in updated_dropped_subjects[sub].keys():\n",
    "        updated_exclusion_df.loc[f'{sub}_{task}', updated_dropped_subjects[sub][task]] = 1\n",
    "\n",
    "        \n",
    "updated_exclusion_df = updated_exclusion_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_exclusion_df.to_csv(\"aim1_beh_subject_exclusions_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Drop Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tasks = ['ANT', 'discountFix', 'DPX', 'motorSelectiveStop', 'stopSignal', \\\n",
    "         'twoByTwo', 'stroop', 'WATT', 'CCTHot', 'surveyMedley']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "\n",
    "subjects = list(df.index.values)\n",
    "subjects.remove('overall_mean')\n",
    "subjects.remove('overall_std')\n",
    "\n",
    "# creates a dictonary of subjects and all tasks\n",
    "for subject in subjects:\n",
    "    metrics[subject] = tasks    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterates through the dropped subjects dict and removes the dropped tasks from metrics\n",
    "for subject in updated_dropped_subjects.keys():\n",
    "    for task in updated_dropped_subjects[subject].keys():\n",
    "        lst = metrics[subject]\n",
    "        lst = list(filter(lambda x: x != task, lst))\n",
    "        metrics.update({subject: lst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty datasets:  19\n",
      "Number of non-datasets arrays:  91\n"
     ]
    }
   ],
   "source": [
    "# obtains the number of complete datasets\n",
    "first_element_of_non_empty = [l[0] for l in metrics.values() if l]\n",
    "\n",
    "num_non_empty = len(first_element_of_non_empty)\n",
    "num_empty = len(metrics) - num_non_empty\n",
    "print(\"Number of empty datasets: \", num_empty)\n",
    "print(\"Number of non-datasets arrays: \", num_non_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complete datasets:  68\n",
      "Number of non-complete datasets:  42\n",
      "Number of partial datasets:  23\n"
     ]
    }
   ],
   "source": [
    "complete_datasets = [l for l in metrics.values() if set(l) == set(tasks)]\n",
    "num_complete = len(complete_datasets)\n",
    "num_non_complete = len(metrics) - num_complete\n",
    "num_partial = num_non_complete - num_empty\n",
    "print(\"Number of complete datasets: \", num_complete)\n",
    "print(\"Number of non-complete datasets: \", num_non_complete)\n",
    "print(\"Number of partial datasets: \", num_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dropped subjects for ANT:  23\n",
      "Total percentage kept for ANT:  0.7909090909090909\n",
      "Number of dropped subjects for discountFix:  22\n",
      "Total percentage kept for discountFix:  0.8\n",
      "Number of dropped subjects for DPX:  23\n",
      "Total percentage kept for DPX:  0.7909090909090909\n",
      "Number of dropped subjects for motorSelectiveStop:  29\n",
      "Total percentage kept for motorSelectiveStop:  0.7363636363636363\n",
      "Number of dropped subjects for stopSignal:  22\n",
      "Total percentage kept for stopSignal:  0.8\n",
      "Number of dropped subjects for twoByTwo:  22\n",
      "Total percentage kept for twoByTwo:  0.8\n",
      "Number of dropped subjects for stroop:  20\n",
      "Total percentage kept for stroop:  0.8181818181818182\n",
      "Number of dropped subjects for WATT:  21\n",
      "Total percentage kept for WATT:  0.8090909090909091\n",
      "Number of dropped subjects for CCTHot:  20\n",
      "Total percentage kept for CCTHot:  0.8181818181818182\n",
      "Number of dropped subjects for surveyMedley:  19\n",
      "Total percentage kept for surveyMedley:  0.8272727272727273\n"
     ]
    }
   ],
   "source": [
    "task_metrics = {}\n",
    "for task in tasks:\n",
    "    test_list = [task]\n",
    "    res = {idx : sum(1 for i in j if i in test_list) for idx, j in metrics.items()}\n",
    "    \n",
    "    task_occurence = sum(res.values())\n",
    "    total = len(res)\n",
    "\n",
    "    task_metrics[task] = {'percentage': task_occurence/total, 'dropped': total - task_occurence}\n",
    "    \n",
    "for task in task_metrics.keys():\n",
    "    print(f'Number of dropped subjects for {task}: ', task_metrics[task]['dropped'])\n",
    "    print(f'Total percentage kept for {task}: ', task_metrics[task]['percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metrics = {\n",
    "    'completed datasets': [num_complete],\n",
    "    'fully dropped datasets': [num_empty],\n",
    "    'partial datasets': [num_partial],\n",
    "    'total': [total],\n",
    "}\n",
    "\n",
    "for task in tasks:\n",
    "    dataset_metrics.update({f'{task} dropped': [task_metrics[task]['dropped']]})\n",
    "    dataset_metrics.update({f'{task} kept': [total - task_metrics[task]['dropped']]})\n",
    "    dataset_metrics.update({f'{task} percentage kept': [task_metrics[task]['percentage']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aim1 beh data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>completed datasets</th>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fully dropped datasets</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial datasets</th>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANT dropped</th>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANT kept</th>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANT percentage kept</th>\n",
       "      <td>0.790909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discountFix dropped</th>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discountFix kept</th>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discountFix percentage kept</th>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DPX dropped</th>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DPX kept</th>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DPX percentage kept</th>\n",
       "      <td>0.790909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorSelectiveStop dropped</th>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorSelectiveStop kept</th>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motorSelectiveStop percentage kept</th>\n",
       "      <td>0.736364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopSignal dropped</th>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopSignal kept</th>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopSignal percentage kept</th>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twoByTwo dropped</th>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twoByTwo kept</th>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twoByTwo percentage kept</th>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stroop dropped</th>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stroop kept</th>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stroop percentage kept</th>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WATT dropped</th>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WATT kept</th>\n",
       "      <td>89.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WATT percentage kept</th>\n",
       "      <td>0.809091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCTHot dropped</th>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCTHot kept</th>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCTHot percentage kept</th>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surveyMedley dropped</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surveyMedley kept</th>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surveyMedley percentage kept</th>\n",
       "      <td>0.827273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    aim1 beh data\n",
       "completed datasets                  68.000000    \n",
       "fully dropped datasets              19.000000    \n",
       "partial datasets                    23.000000    \n",
       "total                               110.000000   \n",
       "ANT dropped                         23.000000    \n",
       "ANT kept                            87.000000    \n",
       "ANT percentage kept                 0.790909     \n",
       "discountFix dropped                 22.000000    \n",
       "discountFix kept                    88.000000    \n",
       "discountFix percentage kept         0.800000     \n",
       "DPX dropped                         23.000000    \n",
       "DPX kept                            87.000000    \n",
       "DPX percentage kept                 0.790909     \n",
       "motorSelectiveStop dropped          29.000000    \n",
       "motorSelectiveStop kept             81.000000    \n",
       "motorSelectiveStop percentage kept  0.736364     \n",
       "stopSignal dropped                  22.000000    \n",
       "stopSignal kept                     88.000000    \n",
       "stopSignal percentage kept          0.800000     \n",
       "twoByTwo dropped                    22.000000    \n",
       "twoByTwo kept                       88.000000    \n",
       "twoByTwo percentage kept            0.800000     \n",
       "stroop dropped                      20.000000    \n",
       "stroop kept                         90.000000    \n",
       "stroop percentage kept              0.818182     \n",
       "WATT dropped                        21.000000    \n",
       "WATT kept                           89.000000    \n",
       "WATT percentage kept                0.809091     \n",
       "CCTHot dropped                      20.000000    \n",
       "CCTHot kept                         90.000000    \n",
       "CCTHot percentage kept              0.818182     \n",
       "surveyMedley dropped                19.000000    \n",
       "surveyMedley kept                   91.000000    \n",
       "surveyMedley percentage kept        0.827273     "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_info = pd.DataFrame.from_dict(dataset_metrics)\n",
    "dataset_info.rename(index={0:'aim1 beh data'},inplace=True)\n",
    "dataset_info.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info.T.to_csv(\"aim1_dataset_metrics_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling Subject List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped_subjects dictionary\n",
    "with open('/home/groups/russpold/uh2_analysis/jaime_wonderland/Aim1_Neuro/Individual Neuro Analysis/aim1_dropped_dict.pickle', 'wb') as fh:\n",
    "   pickle.dump(dropped_subjects, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped_subjects dictionary\n",
    "with open('/home/groups/russpold/uh2_analysis/jaime_wonderland/Aim1_Neuro/Individual Neuro Analysis/aim1_metrics.pickle', 'wb') as fh:\n",
    "   pickle.dump(metrics, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching mapping for s644 Stroop task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining df\n",
    "file = '/oak/stanford/groups/russpold/data/uh2/aim1/behavioral_data/processed/s644_stroop_cleaned.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# only looking at testing phase\n",
    "subj_df =  df.loc[(df.loc[:,'exp_stage'] == 'test')]\n",
    "\n",
    "# swapping mapping\n",
    "subj_df.loc[subj_df['key_press'] == 71, 'key_press'] = 43\n",
    "subj_df.loc[subj_df['key_press'] == 82, 'key_press'] = 71\n",
    "subj_df.loc[subj_df['key_press'] == 43, 'key_press'] = 82\n",
    "\n",
    "# adjusting correct columns to reflect\n",
    "correct = (subj_df['key_press'] == subj_df['correct_response'])\n",
    "subj_df['correct'][correct] = 1\n",
    "incorrect = (subj_df['key_press'] != subj_df['correct_response'])\n",
    "subj_df['correct'][incorrect] = 0\n",
    "\n",
    "# calculating new accuracy\n",
    "response_df = subj_df[subj_df.key_press != -1]\n",
    "congruent = response_df[response_df.condition == 'congruent']\n",
    "inc = response_df[response_df.condition == 'incongruent']\n",
    "inc_acc = sum(inc['correct']) / len(inc['correct']) \n",
    "con_acc = sum(congruent['correct']) / len(congruent['correct']) \n",
    "print(f'con acc: {con_acc} // incon acc: {inc_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining beh and brain qc for aim 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output'\n",
    "tasks = ['ANT', 'discountFix', 'DPX', 'motorSelectiveStop', 'stopSignal', \\\n",
    "         'twoByTwo', 'stroop', 'WATT3', 'CCTHot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "for task in tasks:\n",
    "    if task in ['WATT3', 'CCTHot']:\n",
    "        files[task] = glob(path.join(data_dir, f'{task}_lev1_output', f'task_{task}_rtmodel_no_rt', f'excluded_subject.csv'))\n",
    "    else:\n",
    "        files[task] = glob(path.join(data_dir, f'{task}_lev1_output', f'task_{task}_rtmodel_rt_centered', f'excluded_subject.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANT': ['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/ANT_lev1_output/task_ANT_rtmodel_rt_centered/excluded_subject.csv'],\n",
       " 'discountFix': ['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/discountFix_lev1_output/task_discountFix_rtmodel_rt_centered/excluded_subject.csv'],\n",
       " 'DPX': ['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/DPX_lev1_output/task_DPX_rtmodel_rt_centered/excluded_subject.csv'],\n",
       " 'motorSelectiveStop': ['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/motorSelectiveStop_lev1_output/task_motorSelectiveStop_rtmodel_rt_centered/excluded_subject.csv'],\n",
       " 'stopSignal': ['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/stopSignal_lev1_output/task_stopSignal_rtmodel_rt_centered/excluded_subject.csv'],\n",
       " 'twoByTwo': ['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/twoByTwo_lev1_output/task_twoByTwo_rtmodel_rt_centered/excluded_subject.csv'],\n",
       " 'stroop': ['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/stroop_lev1_output/task_stroop_rtmodel_rt_centered/excluded_subject.csv'],\n",
       " 'WATT3': ['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/WATT3_lev1_output/task_WATT3_rtmodel_no_rt/excluded_subject.csv'],\n",
       " 'CCTHot': ['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/CCTHot_lev1_output/task_CCTHot_rtmodel_no_rt/excluded_subject.csv']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/oak/stanford/groups/russpold/data/uh2/aim1/BIDS/derivatives/output/ANT_lev1_output/task_ANT_rtmodel_rt_centered/excluded_subject.csv']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files['ANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusions = pd.read_csv(files['ANT'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subid_task</th>\n",
       "      <th>truncated_more_than_half_data</th>\n",
       "      <th>poor_performance_subjective_rating</th>\n",
       "      <th>greater_than_50_percent_nonresponse_rate</th>\n",
       "      <th>more_than_75_percent_stop_sucess_rate</th>\n",
       "      <th>less_than_25_percent_stop_sucess_rate</th>\n",
       "      <th>missing_beh_data</th>\n",
       "      <th>missing_more_than_half_the_tasks</th>\n",
       "      <th>percent_junk_gt_45</th>\n",
       "      <th>percent_scrub_gt_20</th>\n",
       "      <th>task_related_regressor_all_zeros</th>\n",
       "      <th>event_file_missing</th>\n",
       "      <th>confounds_file_missing</th>\n",
       "      <th>mask_file_missing</th>\n",
       "      <th>data_file_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>623_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>061_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>607_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>609_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>599_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>600_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>644_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>592_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>635_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.226714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>526_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>637_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>650_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>144_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>640_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>583_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>646_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>533_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>603_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>639_ANT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subid_task  truncated_more_than_half_data  \\\n",
       "0   623_ANT    0.0                             \n",
       "1   061_ANT    0.0                             \n",
       "2   607_ANT    0.0                             \n",
       "3   609_ANT    0.0                             \n",
       "4   599_ANT    0.0                             \n",
       "5   600_ANT    0.0                             \n",
       "6   644_ANT    0.0                             \n",
       "7   592_ANT    0.0                             \n",
       "8   635_ANT    0.0                             \n",
       "9   526_ANT    0.0                             \n",
       "10  637_ANT    0.0                             \n",
       "11  650_ANT    0.0                             \n",
       "12  144_ANT    0.0                             \n",
       "13  640_ANT    0.0                             \n",
       "14  583_ANT    0.0                             \n",
       "15  646_ANT    0.0                             \n",
       "16  533_ANT    0.0                             \n",
       "17  603_ANT    0.0                             \n",
       "18  639_ANT    0.0                             \n",
       "\n",
       "    poor_performance_subjective_rating  \\\n",
       "0   0.0                                  \n",
       "1   0.0                                  \n",
       "2   0.0                                  \n",
       "3   0.0                                  \n",
       "4   0.0                                  \n",
       "5   0.0                                  \n",
       "6   0.0                                  \n",
       "7   0.0                                  \n",
       "8   0.0                                  \n",
       "9   0.0                                  \n",
       "10  0.0                                  \n",
       "11  1.0                                  \n",
       "12  0.0                                  \n",
       "13  0.0                                  \n",
       "14  0.0                                  \n",
       "15  0.0                                  \n",
       "16  0.0                                  \n",
       "17  0.0                                  \n",
       "18  0.0                                  \n",
       "\n",
       "    greater_than_50_percent_nonresponse_rate  \\\n",
       "0   0.0                                        \n",
       "1   0.0                                        \n",
       "2   0.0                                        \n",
       "3   0.0                                        \n",
       "4   0.0                                        \n",
       "5   0.0                                        \n",
       "6   0.0                                        \n",
       "7   0.0                                        \n",
       "8   1.0                                        \n",
       "9   0.0                                        \n",
       "10  0.0                                        \n",
       "11  0.0                                        \n",
       "12  0.0                                        \n",
       "13  0.0                                        \n",
       "14  0.0                                        \n",
       "15  0.0                                        \n",
       "16  0.0                                        \n",
       "17  0.0                                        \n",
       "18  0.0                                        \n",
       "\n",
       "    more_than_75_percent_stop_sucess_rate  \\\n",
       "0   0.0                                     \n",
       "1   0.0                                     \n",
       "2   0.0                                     \n",
       "3   0.0                                     \n",
       "4   0.0                                     \n",
       "5   0.0                                     \n",
       "6   0.0                                     \n",
       "7   0.0                                     \n",
       "8   0.0                                     \n",
       "9   0.0                                     \n",
       "10  0.0                                     \n",
       "11  0.0                                     \n",
       "12  0.0                                     \n",
       "13  0.0                                     \n",
       "14  0.0                                     \n",
       "15  0.0                                     \n",
       "16  0.0                                     \n",
       "17  0.0                                     \n",
       "18  0.0                                     \n",
       "\n",
       "    less_than_25_percent_stop_sucess_rate  missing_beh_data  \\\n",
       "0   0.0                                    0.0                \n",
       "1   0.0                                    1.0                \n",
       "2   0.0                                    1.0                \n",
       "3   0.0                                    1.0                \n",
       "4   0.0                                    1.0                \n",
       "5   0.0                                    1.0                \n",
       "6   0.0                                    0.0                \n",
       "7   0.0                                    1.0                \n",
       "8   0.0                                    0.0                \n",
       "9   0.0                                    0.0                \n",
       "10  0.0                                    1.0                \n",
       "11  0.0                                    0.0                \n",
       "12  0.0                                    0.0                \n",
       "13  0.0                                    0.0                \n",
       "14  0.0                                    0.0                \n",
       "15  0.0                                    0.0                \n",
       "16  0.0                                    0.0                \n",
       "17  0.0                                    0.0                \n",
       "18  0.0                                    0.0                \n",
       "\n",
       "    missing_more_than_half_the_tasks  percent_junk_gt_45  percent_scrub_gt_20  \\\n",
       "0   1.0                               0.000000            0.000000              \n",
       "1   0.0                               0.000000            0.000000              \n",
       "2   0.0                               0.000000            0.000000              \n",
       "3   0.0                               0.000000            0.000000              \n",
       "4   1.0                               0.000000            0.000000              \n",
       "5   1.0                               0.000000            0.000000              \n",
       "6   0.0                               0.000000            0.000000              \n",
       "7   1.0                               0.000000            0.000000              \n",
       "8   0.0                               0.726562            0.226714              \n",
       "9   0.0                               0.000000            0.420664              \n",
       "10  1.0                               0.000000            0.000000              \n",
       "11  1.0                               0.000000            0.000000              \n",
       "12  1.0                               0.000000            0.000000              \n",
       "13  1.0                               0.000000            0.000000              \n",
       "14  0.0                               0.000000            0.257785              \n",
       "15  1.0                               0.000000            0.000000              \n",
       "16  1.0                               0.000000            0.000000              \n",
       "17  1.0                               0.000000            0.000000              \n",
       "18  1.0                               0.000000            0.000000              \n",
       "\n",
       "    task_related_regressor_all_zeros  event_file_missing  \\\n",
       "0   0.0                               0.0                  \n",
       "1   0.0                               1.0                  \n",
       "2   0.0                               1.0                  \n",
       "3   0.0                               1.0                  \n",
       "4   0.0                               1.0                  \n",
       "5   0.0                               1.0                  \n",
       "6   0.0                               0.0                  \n",
       "7   0.0                               1.0                  \n",
       "8   0.0                               0.0                  \n",
       "9   0.0                               0.0                  \n",
       "10  0.0                               1.0                  \n",
       "11  0.0                               0.0                  \n",
       "12  0.0                               0.0                  \n",
       "13  0.0                               0.0                  \n",
       "14  0.0                               0.0                  \n",
       "15  0.0                               0.0                  \n",
       "16  0.0                               0.0                  \n",
       "17  0.0                               0.0                  \n",
       "18  0.0                               0.0                  \n",
       "\n",
       "    confounds_file_missing  mask_file_missing  data_file_missing  \n",
       "0   0.0                     0.0                0.0                \n",
       "1   1.0                     1.0                1.0                \n",
       "2   1.0                     1.0                1.0                \n",
       "3   1.0                     1.0                1.0                \n",
       "4   1.0                     1.0                1.0                \n",
       "5   1.0                     1.0                1.0                \n",
       "6   1.0                     1.0                1.0                \n",
       "7   0.0                     0.0                0.0                \n",
       "8   0.0                     0.0                0.0                \n",
       "9   0.0                     0.0                0.0                \n",
       "10  1.0                     1.0                1.0                \n",
       "11  0.0                     0.0                0.0                \n",
       "12  0.0                     0.0                0.0                \n",
       "13  0.0                     0.0                0.0                \n",
       "14  0.0                     0.0                0.0                \n",
       "15  0.0                     0.0                0.0                \n",
       "16  0.0                     0.0                0.0                \n",
       "17  0.0                     0.0                0.0                \n",
       "18  0.0                     0.0                0.0                "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
